{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import multiprocessing\n",
    "import platform\n",
    "import torch\n",
    "import numpy as np\n",
    "import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Subset\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "import time\n",
    "import os\n",
    "# import tqdm\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "from utils.normalization import normalize_range, un_normalize_range\n",
    "from utils.seed import set_seed\n",
    "\n",
    "from sequential_nn.dataset import GluMemDataset_4pool\n",
    "from sequential_nn.model import Network\n",
    "from sequential_nn.multi_dict import pkl_2_dat\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T21:03:04.413355400Z",
     "start_time": "2024-07-29T21:02:00.295320100Z"
    }
   },
   "id": "ff0f2e0f1adc82dc",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single dict case\n",
      "Total time taken for memmap generation: 4 minutes 28 seconds\n",
      "Single dict case\n",
      "Total time taken for memmap generation: 6 minutes 32 seconds\n"
     ]
    }
   ],
   "source": [
    "glu_dict_upper_folder_fn = f'D:\\mrf\\data\\exp\\mt_amide_glu_dicts\\high_glu_conc_500\\glu'\n",
    "glu_dict_folder_fn = os.path.join(glu_dict_upper_folder_fn, '107a')\n",
    "memmap_fn = os.path.join(glu_dict_folder_fn, 'M0_dict.dat')\n",
    "if not os.path.exists(memmap_fn):\n",
    "    pkl_2_dat(glu_dict_folder_fn, sched_iter=30, add_iter=6, memmap_fn=memmap_fn, M0_flag=True)\n",
    "glu_dict_folder_fn = os.path.join(glu_dict_upper_folder_fn, '51_Amide')\n",
    "memmap_fn = os.path.join(glu_dict_folder_fn, 'M0_dict.dat')\n",
    "if not os.path.exists(memmap_fn):\n",
    "    pkl_2_dat(glu_dict_folder_fn, sched_iter=30, add_iter=6, memmap_fn=memmap_fn, M0_flag=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T19:39:09.090597300Z",
     "start_time": "2024-07-29T19:28:06.675465Z"
    }
   },
   "id": "6af7942ad4bb6e42",
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single dict case\n",
      "Total time taken for memmap generation: 0 minutes 2 seconds\n"
     ]
    }
   ],
   "source": [
    "glu_dict_upper_folder_fn = r'D:\\mrf\\data\\exp\\mt_amide_glu_dicts\\high_glu_conc_500\\amide'\n",
    "glu_dict_folder_fn = os.path.join(glu_dict_upper_folder_fn, '51_Amide')\n",
    "memmap_fn = os.path.join(glu_dict_folder_fn, 'M0_dict.dat')\n",
    "if not os.path.exists(memmap_fn):\n",
    "    pkl_2_dat(glu_dict_folder_fn, sched_iter=30, add_iter=4, memmap_fn=memmap_fn, M0_flag=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T19:43:42.061090600Z",
     "start_time": "2024-07-29T19:43:39.922857Z"
    }
   },
   "id": "2a7fd1371c6b8f72",
   "execution_count": 13
  },
  {
   "cell_type": "code",
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./runs --port 6007"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-29T11:44:42.508569600Z",
     "start_time": "2024-07-29T11:44:42.404162Z"
    }
   },
   "id": "f37ee3b0d89ea3c1",
   "outputs": [
    {
     "data": {
      "text/plain": "Launching TensorBoard..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n      <iframe id=\"tensorboard-frame-24346de0a7e8c7d5\" width=\"100%\" height=\"800\" frameborder=\"0\">\n      </iframe>\n      <script>\n        (function() {\n          const frame = document.getElementById(\"tensorboard-frame-24346de0a7e8c7d5\");\n          const url = new URL(\"/\", window.location);\n          const port = 6007;\n          if (port) {\n            url.port = port;\n          }\n          frame.src = url;\n        })();\n      </script>\n    "
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "def main():\n",
    "    torch.multiprocessing.freeze_support()\n",
    "    dict_name_category = 'high_glu_conc_500'\n",
    "    fp_prtcl_name = '107a'\n",
    "\n",
    "    # Schedule iterations\n",
    "    # number of raw images in the CEST-MRF acquisition schedule\n",
    "    sched_iter = 30\n",
    "    add_iter = 6\n",
    "\n",
    "    # Training properties\n",
    "    learning_rate = 2e-4\n",
    "    step_size = 1\n",
    "    gamma = 1\n",
    "    batch_size = 1024\n",
    "    num_epochs = 10  # 150\n",
    "    noise_std = 1e-3  # noise level for training, 1e-2\n",
    "\n",
    "    min_delta = 0.05  # minimum absolute change in the loss function\n",
    "    patience = np.inf\n",
    "\n",
    "    current_dir = os.getcwd()  # Get the current directory\n",
    "    parent_dir = os.path.dirname(current_dir)  # Navigate up one directory level\n",
    "    glu_dict_folder_fn = os.path.join(parent_dir, 'data', 'exp', 'mt_amide_glu_dicts', dict_name_category, 'glu',\n",
    "                                      fp_prtcl_name)  # dict folder directory\n",
    "    memmap_fn = os.path.join(glu_dict_folder_fn, 'M0_dict.dat')\n",
    "    glu_dict_fn = os.path.join(glu_dict_folder_fn, 'dict.pkl')\n",
    "\n",
    "    if not os.path.exists(memmap_fn):\n",
    "        pkl_2_dat(glu_dict_folder_fn, sched_iter, add_iter, memmap_fn)\n",
    "\n",
    "    net_name = f'{dict_name_category}_glu_dict_noise_{noise_std}_lr_{learning_rate}_{batch_size}'  # _cosine\n",
    "    nn_fn = os.path.join(current_dir, 'mouse_nns', 'glu_amide_mt_nns', dict_name_category, 'glu', fp_prtcl_name,\n",
    "                         f'M0_{net_name}.pt')  # nn directory\n",
    "\n",
    "    device = initialize_device()\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load the entire dataset to get its size\n",
    "    # full_dataset = Dataset_4pool(glu_dict_fn)\n",
    "    full_dataset = GluMemDataset_4pool(memmap_fn, sched_iter, add_iter)\n",
    "    # full_dataset = NoShuffleMultiDataset(glu_dict_folder_fn, add_iter)\n",
    "    \n",
    "    (min_param_tensor, max_param_tensor,\n",
    "    min_water_t1t2_tensor, max_water_t1t2_tensor,\n",
    "    min_mt_param_tensor, max_mt_param_tensor, \n",
    "    min_amide_param_tensor, max_amide_param_tensor) = define_min_max(memmap_fn, sched_iter, add_iter, device)\n",
    "\n",
    "    # Convert tensors to numpy arrays\n",
    "    min_param_array = min_param_tensor.cpu().numpy()\n",
    "    max_param_array = max_param_tensor.cpu().numpy()\n",
    "    min_water_t1t2_array = min_water_t1t2_tensor.cpu().numpy()\n",
    "    max_water_t1t2_array = max_water_t1t2_tensor.cpu().numpy()\n",
    "    min_mt_param_array = min_mt_param_tensor.cpu().numpy()\n",
    "    max_mt_param_array = max_mt_param_tensor.cpu().numpy()\n",
    "    min_amide_param_array = min_amide_param_tensor.cpu().numpy()\n",
    "    max_amide_param_array = max_amide_param_tensor.cpu().numpy()\n",
    "    \n",
    "    if not os.path.exists(os.path.dirname(nn_fn)):\n",
    "        os.makedirs(os.path.dirname(nn_fn))\n",
    "        \n",
    "    # Save all arrays to a single .npz file\n",
    "    np.savez(os.path.join(os.path.dirname(nn_fn),'min_max_values.npz'),\n",
    "             min_param=min_param_array,\n",
    "             max_param=max_param_array,\n",
    "             min_water_t1t2=min_water_t1t2_array,\n",
    "             max_water_t1t2=max_water_t1t2_array,\n",
    "             min_mt_param=min_mt_param_array,\n",
    "             max_mt_param=max_mt_param_array,\n",
    "             min_amide_param=min_amide_param_array,\n",
    "             max_amide_param=max_amide_param_array)\n",
    "    \n",
    "    dataset_size = len(full_dataset)\n",
    "\n",
    "    # Split indices for training, validation, and test sets\n",
    "    train_indices, val_indices, test_indices = split_dataset_indices(dataset_size, val_ratio=0.2, test_ratio=0.1)\n",
    "\n",
    "    # Create subsets\n",
    "    train_dataset = Subset(full_dataset, train_indices)\n",
    "    val_dataset = Subset(full_dataset, val_indices)\n",
    "    test_dataset = Subset(full_dataset, test_indices)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=1)\n",
    "\n",
    "    val_loader = DataLoader(dataset=val_dataset,\n",
    "                            batch_size=batch_size,\n",
    "                            shuffle=False,\n",
    "                            num_workers=1)\n",
    "\n",
    "    test_loader = DataLoader(dataset=test_dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=False,\n",
    "                             num_workers=1)\n",
    "\n",
    "    train_network(train_loader, val_loader, test_loader, device, sched_iter, add_iter, dict_name_category, learning_rate, num_epochs, noise_std, patience,\n",
    "                  min_delta, min_param_tensor, max_param_tensor, min_water_t1t2_tensor,\n",
    "                  max_water_t1t2_tensor, min_mt_param_tensor, max_mt_param_tensor, min_amide_param_tensor, max_amide_param_tensor, nn_fn, step_size, gamma, net_name)\n",
    "\n",
    "\n",
    "# Function to split dataset indices\n",
    "def split_dataset_indices(dataset_size, val_ratio=0.2, test_ratio=0.1):\n",
    "    indices = np.arange(dataset_size)\n",
    "    np.random.shuffle(indices)\n",
    "    test_split = int(test_ratio * dataset_size)\n",
    "    val_split = int(val_ratio * dataset_size) + test_split\n",
    "    test_indices = indices[:test_split]\n",
    "    val_indices = indices[test_split:val_split]\n",
    "    train_indices = indices[val_split:]\n",
    "    return train_indices, val_indices, test_indices\n",
    "\n",
    "# Function to initialize device\n",
    "def initialize_device():\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# Function to train the network\n",
    "def train_network(train_loader, val_loader, test_loader, device, sched_iter, add_iter, dict_name, learning_rate, num_epochs, noise_std, patience, min_delta,\n",
    "                  min_param_tensor, max_param_tensor, min_water_t1t2_tensor, max_water_t1t2_tensor,\n",
    "                  min_mt_param_fs_ksw, max_mt_param_fs_ksw,  min_amide_param_fs_ksw, max_amide_param_fs_ksw, nn_fn, step_size, gamma, net_name):\n",
    "    nn_folder = os.path.dirname(nn_fn)  # Navigate up one directory level\n",
    "    if not os.path.exists(nn_folder):\n",
    "        os.makedirs(nn_folder)\n",
    "\n",
    "    # Initializing the reconstruction network\n",
    "    reco_net = Network(sched_iter, add_iter=add_iter, n_hidden=2, n_neurons=300).to(device)\n",
    "\n",
    "    # Print amount of parameters\n",
    "    print('Number of model parameters: ', sum(p.numel() for p in reco_net.parameters() if p.requires_grad))\n",
    "\n",
    "    # Setting optimizer\n",
    "    optimizer = torch.optim.Adam(reco_net.parameters(), lr=learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    # Storing current time\n",
    "    t0 = time.time()\n",
    "    # Get today's date\n",
    "    today = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "    writer = SummaryWriter(log_dir=f'runs/{net_name}')\n",
    "\n",
    "    loss_per_epoch = []\n",
    "    val_loss_per_epoch = []\n",
    "    patience_counter = 0\n",
    "    min_loss = 100\n",
    "\n",
    "    reco_net.train()\n",
    "    cur_val_loss = float('inf')\n",
    "\n",
    "    pbar = tqdm.tqdm(total=num_epochs)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Cumulative loss\n",
    "        cum_loss = 0\n",
    "        counter = np.nan\n",
    "        \n",
    "        num_steps = len(train_loader)\n",
    "        inner_pbar = tqdm.tqdm(total=num_steps)\n",
    "        for counter, dict_params in enumerate(train_loader, 0):\n",
    "            reco_net, cum_loss = train_step(device, noise_std, reco_net, optimizer, cum_loss, dict_params,\n",
    "                                            min_param_tensor, max_param_tensor,\n",
    "                                            min_water_t1t2_tensor, max_water_t1t2_tensor,\n",
    "                                            min_mt_param_fs_ksw, max_mt_param_fs_ksw, min_amide_param_fs_ksw, max_amide_param_fs_ksw, \n",
    "                                            writer, epoch, counter, num_steps)\n",
    "            inner_pbar.set_description(f'Step: {counter+1}/{num_steps}')\n",
    "            inner_pbar.update(1)\n",
    "            \n",
    "            del dict_params\n",
    "            torch.cuda.empty_cache()\n",
    "        inner_pbar.close()\n",
    "\n",
    "        # Average loss for this epoch\n",
    "        loss_per_epoch.append(cum_loss / (counter + 1))\n",
    "        \n",
    "        # Validate the model\n",
    "        val_loss = validate(reco_net, val_loader, device, min_param_tensor, max_param_tensor,\n",
    "                            min_water_t1t2_tensor, max_water_t1t2_tensor,\n",
    "                            min_mt_param_fs_ksw, max_mt_param_fs_ksw, min_amide_param_fs_ksw, max_amide_param_fs_ksw)\n",
    "        val_loss_per_epoch.append(val_loss)\n",
    "        \n",
    "        writer.add_scalar(\"Loss/train\", loss_per_epoch[-1], epoch)\n",
    "        writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "\n",
    "        pbar.set_description(f'Epoch: {epoch + 1}/{num_epochs}, Train Loss = {loss_per_epoch[-1]}, Val Loss = {val_loss_per_epoch[-1]}')\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if (min_loss - val_loss_per_epoch[-1]) / min_loss > min_delta:\n",
    "            min_loss = val_loss_per_epoch[-1]\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter > patience:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "            \n",
    "        # Scheduler step\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save model checkpoint when val loss gets better\n",
    "        if val_loss <= cur_val_loss:\n",
    "            print(f\"\\nSaved epoch {epoch} model\")\n",
    "            torch.save({\n",
    "                'model_state_dict': reco_net.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss_per_epoch': loss_per_epoch,\n",
    "                'val_loss_per_epoch': val_loss_per_epoch,\n",
    "                'noise_std': noise_std,\n",
    "                'epoch': epoch\n",
    "            }, nn_fn)\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            cur_val_loss = val_loss\n",
    "\n",
    "    pbar.close()\n",
    "    print(f\"Training took {time.time() - t0:.2f} seconds\")\n",
    "\n",
    "    # # Save final model checkpoint\n",
    "    # torch.save({\n",
    "    #     'model_state_dict': reco_net.state_dict(),\n",
    "    #     'optimizer_state_dict': optimizer.state_dict(),\n",
    "    #     'loss_per_epoch': loss_per_epoch,\n",
    "    #     'val_loss_per_epoch': val_loss_per_epoch,\n",
    "    #     'noise_std': noise_std,\n",
    "    # }, nn_fn)\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "    # Test the model\n",
    "    test_loss = test(reco_net, test_loader, device, min_param_tensor, max_param_tensor,\n",
    "                     min_water_t1t2_tensor, max_water_t1t2_tensor,\n",
    "                     min_mt_param_fs_ksw, max_mt_param_fs_ksw, min_amide_param_fs_ksw, max_amide_param_fs_ksw)\n",
    "    print(f\"Test Loss: {test_loss}\")\n",
    "\n",
    "    return reco_net\n",
    "\n",
    "def train_step(device, noise_std, reco_net, optimizer, cum_loss, dict_params, min_param_tensor, max_param_tensor,\n",
    "               min_water_t1t2_tensor, max_water_t1t2_tensor, min_mt_param_fs_ksw, max_mt_param_fs_ksw, min_amide_param_fs_ksw, max_amide_param_fs_ksw,\n",
    "               writer, epoch, counter, num_steps):\n",
    "    cur_fs, cur_ksw, cur_t1w, cur_t2w, cur_mt_fs, cur_mt_ksw, cur_amide_fs, cur_amide_ksw, cur_norm_sig = dict_params\n",
    "\n",
    "    target = torch.stack((cur_fs, cur_ksw), dim=1).to(device)\n",
    "    input_water_t1t2 = torch.stack((cur_t1w, cur_t2w), dim=1).to(device)\n",
    "    input_mt_fs_ksw = torch.stack((cur_mt_fs, cur_mt_ksw), dim=1).to(device)\n",
    "    input_amide_fs_ksw = torch.stack((cur_amide_fs, cur_amide_ksw), dim=1).to(device)\n",
    "\n",
    "    # Normalizing the target and input_water_t1t2\n",
    "    target = normalize_range(original_array=target, original_min=min_param_tensor,\n",
    "                             original_max=max_param_tensor, new_min=0, new_max=1).to(device)\n",
    "\n",
    "    input_water_t1t2 = normalize_range(original_array=input_water_t1t2, original_min=min_water_t1t2_tensor,\n",
    "                                       original_max=max_water_t1t2_tensor, new_min=0, new_max=1).to(device)\n",
    "\n",
    "    input_mt_fs_ksw = normalize_range(original_array=input_mt_fs_ksw, original_min=min_mt_param_fs_ksw,\n",
    "                                      original_max=max_mt_param_fs_ksw, new_min=0, new_max=1).to(device)\n",
    "    \n",
    "    input_amide_fs_ksw = normalize_range(original_array=input_amide_fs_ksw, original_min=min_amide_param_fs_ksw,\n",
    "                                  original_max=max_amide_param_fs_ksw, new_min=0, new_max=1).to(device)\n",
    "\n",
    "    # Adding noise to the input signals (trajectories)\n",
    "    noised_sig = cur_norm_sig + torch.randn(cur_norm_sig.size()) * noise_std\n",
    "\n",
    "    # adding the mt_fs_ksw and t1, t2 as additional nn input\n",
    "    noised_sig = torch.hstack((input_amide_fs_ksw, input_mt_fs_ksw, input_water_t1t2, noised_sig.to(device))).to(device)\n",
    "    del input_water_t1t2, input_mt_fs_ksw, input_amide_fs_ksw\n",
    "\n",
    "    # Forward step\n",
    "    prediction = reco_net(noised_sig.float())\n",
    "    del noised_sig\n",
    "\n",
    "    # Batch loss (MSE)\n",
    "    loss = torch.mean((prediction.float() - target.float()) ** 2)\n",
    "    del target\n",
    "\n",
    "    # Backward step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimization step\n",
    "    optimizer.step()\n",
    "\n",
    "    # Storing Cumulative loss\n",
    "    cum_loss += loss.item()\n",
    "    \n",
    "    writer.add_scalar(\"Loss/train_step\", loss.item(), counter+epoch*num_steps)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return reco_net, cum_loss\n",
    "\n",
    "\n",
    "def validate(reco_net, val_loader, device, min_param_tensor, max_param_tensor, min_water_t1t2_tensor, max_water_t1t2_tensor,\n",
    "             min_mt_param_fs_ksw, max_mt_param_fs_ksw, min_amide_param_fs_ksw, max_amide_param_fs_ksw):\n",
    "    reco_net.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for dict_params in val_loader:\n",
    "            cur_fs, cur_ksw, cur_t1w, cur_t2w, cur_mt_fs, cur_mt_ksw, cur_amide_fs, cur_amide_ksw, cur_norm_sig = dict_params\n",
    "\n",
    "            target = torch.stack((cur_fs, cur_ksw), dim=1).to(device)\n",
    "            input_water_t1t2 = torch.stack((cur_t1w, cur_t2w), dim=1).to(device)\n",
    "            input_mt_fs_ksw = torch.stack((cur_mt_fs, cur_mt_ksw), dim=1).to(device)\n",
    "            input_amide_fs_ksw = torch.stack((cur_amide_fs, cur_amide_ksw), dim=1).to(device)\n",
    "\n",
    "            target = normalize_range(original_array=target, original_min=min_param_tensor,\n",
    "                                     original_max=max_param_tensor, new_min=0, new_max=1).to(device)\n",
    "            input_water_t1t2 = normalize_range(original_array=input_water_t1t2, original_min=min_water_t1t2_tensor,\n",
    "                                               original_max=max_water_t1t2_tensor, new_min=0, new_max=1).to(device)\n",
    "            input_mt_fs_ksw = normalize_range(original_array=input_mt_fs_ksw, original_min=min_mt_param_fs_ksw,\n",
    "                                              original_max=max_mt_param_fs_ksw, new_min=0, new_max=1).to(device)\n",
    "            input_amide_fs_ksw = normalize_range(original_array=input_amide_fs_ksw, original_min=min_amide_param_fs_ksw,\n",
    "                              original_max=max_amide_param_fs_ksw, new_min=0, new_max=1).to(device)\n",
    "\n",
    "            noised_sig = cur_norm_sig\n",
    "\n",
    "            noised_sig = torch.hstack((input_amide_fs_ksw, input_mt_fs_ksw, input_water_t1t2, noised_sig.to(device))).to(device)\n",
    "\n",
    "            prediction = reco_net(noised_sig.float())\n",
    "\n",
    "            loss = torch.mean((prediction.float() - target.float()) ** 2)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(val_loader)\n",
    "\n",
    "\n",
    "def test(reco_net, test_loader, device, min_param_tensor, max_param_tensor, min_water_t1t2_tensor, max_water_t1t2_tensor,\n",
    "         min_mt_param_fs_ksw, max_mt_param_fs_ksw, min_amide_param_fs_ksw, max_amide_param_fs_ksw):\n",
    "    reco_net.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for dict_params in test_loader:\n",
    "            cur_fs, cur_ksw, cur_t1w, cur_t2w, cur_mt_fs, cur_mt_ksw, cur_amide_fs, cur_amide_ksw, cur_norm_sig = dict_params\n",
    "\n",
    "            target = torch.stack((cur_fs, cur_ksw), dim=1).to(device)\n",
    "            input_water_t1t2 = torch.stack((cur_t1w, cur_t2w), dim=1).to(device)\n",
    "            input_mt_fs_ksw = torch.stack((cur_mt_fs, cur_mt_ksw), dim=1).to(device)\n",
    "            input_amide_fs_ksw = torch.stack((cur_amide_fs, cur_amide_ksw), dim=1).to(device)\n",
    "\n",
    "            target = normalize_range(original_array=target, original_min=min_param_tensor,\n",
    "                                     original_max=max_param_tensor, new_min=0, new_max=1).to(device)\n",
    "            input_water_t1t2 = normalize_range(original_array=input_water_t1t2, original_min=min_water_t1t2_tensor,\n",
    "                                               original_max=max_water_t1t2_tensor, new_min=0, new_max=1).to(device)\n",
    "            input_mt_fs_ksw = normalize_range(original_array=input_mt_fs_ksw, original_min=min_mt_param_fs_ksw,\n",
    "                                              original_max=max_mt_param_fs_ksw, new_min=0, new_max=1).to(device)\n",
    "            input_amide_fs_ksw = normalize_range(original_array=input_amide_fs_ksw, original_min=min_amide_param_fs_ksw,\n",
    "                  original_max=max_amide_param_fs_ksw, new_min=0, new_max=1).to(device)\n",
    "\n",
    "            noised_sig = cur_norm_sig\n",
    "\n",
    "            noised_sig = torch.hstack((input_amide_fs_ksw, input_mt_fs_ksw, input_water_t1t2, noised_sig.to(device))).to(device)\n",
    "\n",
    "            prediction = reco_net(noised_sig.float())\n",
    "\n",
    "            loss = torch.mean((prediction.float() - target.float()) ** 2)\n",
    "\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    return test_loss / len(test_loader)\n",
    "\n",
    "\n",
    "def define_min_max(memmap_fn, sched_iter, add_iter, device):\n",
    "    num_columns = sched_iter + add_iter + 2\n",
    "    memmap_array = np.memmap(memmap_fn, dtype=np.float64, mode='r')\n",
    "    num_rows = memmap_array.size // num_columns  # Calculate the number of rows\n",
    "    memmap_array.shape = (num_rows, num_columns)  # [#, 30+6]\n",
    "\n",
    "    min_fs = np.min(memmap_array[:, 4])  # uncomment if non-zero minimum limit is required\n",
    "    min_ksw = np.min(memmap_array[:, 5].transpose().astype(float))  # uncomment if non-zero minimum limit needed\n",
    "    max_fs = np.max(memmap_array[:, 4])\n",
    "    max_ksw = np.max(memmap_array[:, 5].transpose().astype(float))\n",
    "\n",
    "    min_t1w = np.min(memmap_array[:, 2])\n",
    "    min_t2w = np.min(memmap_array[:, 3].transpose().astype(float))\n",
    "    max_t1w = np.max(memmap_array[:, 2])\n",
    "    max_t2w = np.max(memmap_array[:, 3].transpose().astype(float))\n",
    "\n",
    "    min_mt_fs = np.min(memmap_array[:, 6])\n",
    "    min_mt_ksw = np.min(memmap_array[:, 7].transpose().astype(float))\n",
    "    max_mt_fs = np.max(memmap_array[:, 6])\n",
    "    max_mt_ksw = np.max(memmap_array[:, 7].transpose().astype(float))\n",
    "\n",
    "    min_amine_fs = np.min(memmap_array[:, 0])\n",
    "    min_amine_ksw = np.min(memmap_array[:, 1].transpose().astype(float))\n",
    "    max_amine_fs = np.max(memmap_array[:, 0])\n",
    "    max_amine_ksw = np.max(memmap_array[:, 1].transpose().astype(float))\n",
    "\n",
    "    min_param_tensor = torch.tensor(np.hstack((min_fs, min_ksw)), requires_grad=False).to(device)  # can be switched to  min_fs, min_ksw\n",
    "    max_param_tensor = torch.tensor(np.hstack((max_fs, max_ksw)), requires_grad=False).to(device)\n",
    "\n",
    "    min_water_t1t2_tensor = torch.tensor(np.hstack((min_t1w, min_t2w)), requires_grad=False).to(device)\n",
    "    max_water_t1t2_tensor = torch.tensor(np.hstack((max_t1w, max_t2w)), requires_grad=False).to(device)\n",
    "\n",
    "    min_mt_param_tensor = torch.tensor(np.hstack((min_mt_fs, min_mt_ksw)), requires_grad=False).to(device)  # can be switched to  min_fs, min_ksw\n",
    "    max_mt_param_tensor = torch.tensor(np.hstack((max_mt_fs, max_mt_ksw)), requires_grad=False).to(device)  # can be switched to  min_fs, min_ksw\n",
    "\n",
    "    min_amine_param_tensor = torch.tensor(np.hstack((min_amine_fs, min_amine_ksw)), requires_grad=False).to(device)  # can be switched to  min_fs, min_ksw\n",
    "    max_amine_param_tensor = torch.tensor(np.hstack((max_amine_fs, max_amine_ksw)),\n",
    "                                       requires_grad=False).to(device)  # can be switched to  min_fs, min_ksw\n",
    "\n",
    "    return (min_param_tensor, max_param_tensor, min_water_t1t2_tensor, max_water_t1t2_tensor,\n",
    "            min_mt_param_tensor, max_mt_param_tensor, min_amine_param_tensor, max_amine_param_tensor)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if platform.system() == 'Windows':\n",
    "        multiprocessing.set_start_method('spawn', force=True)\n",
    "    # os.chdir(os.path.dirname(os.path.realpath(__file__)))\n",
    "    set_seed(2024)\n",
    "\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "end_time": "2024-07-29T22:58:50.310110800Z",
     "start_time": "2024-07-29T21:09:38.709837Z"
    }
   },
   "id": "d0eb0c85a326f83f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 2024\n",
      "Using device: cuda\n",
      "There are 24393600 entries in the training dictionary\n",
      "Number of model parameters:  193502\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/10 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f4501d321eaf4fa3ae4f6130a2739879"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16676 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c64e3fac6ed64cac9fdf6774f416b655"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved epoch 0 model\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16676 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b2300a96ba484f079445e6224aa44ad4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved epoch 1 model\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16676 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9f75b7ce8c094c8fa1893fee21777e06"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16676 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ebaa499cd4e9429faea566be62570b61"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved epoch 3 model\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16676 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7fcd7c0db35e4623b8764b373b78566d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16676 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c560143672ff4cf1a3ec8af48bfbfc3f"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16676 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7f863cc6e1a04ae59a4d72fe76ee556e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved epoch 6 model\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16676 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3bdc5113798d4603bc45c18aab6dbae0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saved epoch 7 model\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16676 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "48c690a00c9e42a7bc650c159bd41e8c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/16676 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "10678c27110a4b5cba0bae2c7dfb3cae"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 6425.42 seconds\n",
      "Test Loss: 0.05101222234597939\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# def main():\n",
    "#     torch.multiprocessing.freeze_support()\n",
    "#     dict_name_category = 'high_glu_conc_100'\n",
    "#     fp_prtcl_name = '107a'\n",
    "# \n",
    "#     # Schedule iterations\n",
    "#     # number of raw images in the CEST-MRF acquisition schedule\n",
    "#     sched_iter = 30\n",
    "#     add_iter = 6\n",
    "# \n",
    "#     # Training properties\n",
    "#     learning_rate = 2e-4\n",
    "#     step_size = 1\n",
    "#     gamma = 1\n",
    "#     batch_size = 1024\n",
    "#     num_epochs = 20  # 150\n",
    "#     noise_std = 1e-3  # noise level for training, 1e-2\n",
    "# \n",
    "#     min_delta = 0.05  # minimum absolute change in the loss function\n",
    "#     patience = np.inf\n",
    "# \n",
    "#     current_dir = os.getcwd()  # Get the current directory\n",
    "#     parent_dir = os.path.dirname(current_dir)  # Navigate up one directory level\n",
    "#     glu_dict_folder_fn = os.path.join(parent_dir, 'data', 'exp', 'mt_amide_glu_dicts', dict_name_category, 'glu',\n",
    "#                                       fp_prtcl_name)  # dict folder directory\n",
    "#     memmap_fn = os.path.join(glu_dict_folder_fn, 'dict.dat')\n",
    "#     glu_dict_fn = os.path.join(glu_dict_folder_fn, 'dict.pkl')\n",
    "# \n",
    "#     if not os.path.exists(memmap_fn):\n",
    "#         pkl_2_dat(glu_dict_folder_fn, sched_iter, add_iter, memmap_fn)\n",
    "# \n",
    "#     net_name = f'{dict_name_category}_glu_dict_noise_{noise_std}_lr_{learning_rate}_{batch_size}'  # _cosine\n",
    "#     nn_fn = os.path.join(current_dir, 'mouse_nns', 'glu_amide_mt_nns', dict_name_category, 'glu', fp_prtcl_name,\n",
    "#                          f'{net_name}.pt')  # nn directory\n",
    "# \n",
    "#     device = initialize_device()\n",
    "#     print(f\"Using device: {device}\")\n",
    "# \n",
    "#     # Load the entire dataset to get its size\n",
    "#     # full_dataset = Dataset_4pool(glu_dict_fn)\n",
    "#     full_dataset = GluMemDataset_4pool(memmap_fn, sched_iter, add_iter)\n",
    "#     # full_dataset = NoShuffleMultiDataset(glu_dict_folder_fn, add_iter)\n",
    "#     \n",
    "#     (min_param_tensor, max_param_tensor,\n",
    "#     min_water_t1t2_tensor, max_water_t1t2_tensor,\n",
    "#     min_mt_param_tensor, max_mt_param_tensor, \n",
    "#     min_amide_param_tensor, max_amide_param_tensor) = define_min_max(memmap_fn, sched_iter, add_iter, device)\n",
    "# \n",
    "#     # Convert tensors to numpy arrays\n",
    "#     min_param_array = min_param_tensor.cpu().numpy()\n",
    "#     max_param_array = max_param_tensor.cpu().numpy()\n",
    "#     min_water_t1t2_array = min_water_t1t2_tensor.cpu().numpy()\n",
    "#     max_water_t1t2_array = max_water_t1t2_tensor.cpu().numpy()\n",
    "#     min_mt_param_array = min_mt_param_tensor.cpu().numpy()\n",
    "#     max_mt_param_array = max_mt_param_tensor.cpu().numpy()\n",
    "#     min_amide_param_array = min_amide_param_tensor.cpu().numpy()\n",
    "#     max_amide_param_array = max_amide_param_tensor.cpu().numpy()\n",
    "#     \n",
    "#     if not os.path.exists(os.path.dirname(nn_fn)):\n",
    "#         os.makedirs(os.path.dirname(nn_fn))\n",
    "#         \n",
    "#     # Save all arrays to a single .npz file\n",
    "#     np.savez(os.path.join(os.path.dirname(nn_fn),'min_max_values.npz'),\n",
    "#              min_param=min_param_array,\n",
    "#              max_param=max_param_array,\n",
    "#              min_water_t1t2=min_water_t1t2_array,\n",
    "#              max_water_t1t2=max_water_t1t2_array,\n",
    "#              min_mt_param=min_mt_param_array,\n",
    "#              max_mt_param=max_mt_param_array,\n",
    "#              min_amide_param=min_amide_param_array,\n",
    "#              max_amide_param=max_amide_param_array)\n",
    "#     \n",
    "#     dataset_size = len(full_dataset)\n",
    "# \n",
    "#     # Split indices for training, validation, and test sets\n",
    "#     train_indices, val_indices, test_indices = split_dataset_indices(dataset_size, val_ratio=0.2, test_ratio=0.1)\n",
    "# \n",
    "#     # Create subsets\n",
    "#     train_dataset = Subset(full_dataset, train_indices)\n",
    "#     val_dataset = Subset(full_dataset, val_indices)\n",
    "#     test_dataset = Subset(full_dataset, test_indices)\n",
    "# \n",
    "#     # Create DataLoaders\n",
    "#     train_loader = DataLoader(dataset=train_dataset,\n",
    "#                               batch_size=batch_size,\n",
    "#                               shuffle=True,\n",
    "#                               num_workers=1)\n",
    "# \n",
    "#     val_loader = DataLoader(dataset=val_dataset,\n",
    "#                             batch_size=batch_size,\n",
    "#                             shuffle=False,\n",
    "#                             num_workers=1)\n",
    "# \n",
    "#     test_loader = DataLoader(dataset=test_dataset,\n",
    "#                              batch_size=batch_size,\n",
    "#                              shuffle=False,\n",
    "#                              num_workers=1)\n",
    "# \n",
    "#     train_network(train_loader, val_loader, test_loader, device, sched_iter, add_iter, dict_name_category, learning_rate, num_epochs, noise_std, patience,\n",
    "#                   min_delta, min_param_tensor, max_param_tensor, min_water_t1t2_tensor,\n",
    "#                   max_water_t1t2_tensor, min_mt_param_tensor, max_mt_param_tensor, min_amide_param_tensor, max_amide_param_tensor, nn_fn, step_size, gamma, net_name)\n",
    "# \n",
    "# \n",
    "# # Function to split dataset indices\n",
    "# def split_dataset_indices(dataset_size, val_ratio=0.2, test_ratio=0.1):\n",
    "#     indices = np.arange(dataset_size)\n",
    "#     np.random.shuffle(indices)\n",
    "#     test_split = int(test_ratio * dataset_size)\n",
    "#     val_split = int(val_ratio * dataset_size) + test_split\n",
    "#     test_indices = indices[:test_split]\n",
    "#     val_indices = indices[test_split:val_split]\n",
    "#     train_indices = indices[val_split:]\n",
    "#     return train_indices, val_indices, test_indices\n",
    "# \n",
    "# # Function to initialize device\n",
    "# def initialize_device():\n",
    "#     return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "# \n",
    "# \n",
    "# # Function to train the network\n",
    "# def train_network(train_loader, val_loader, test_loader, device, sched_iter, add_iter, dict_name, learning_rate, num_epochs, noise_std, patience, min_delta,\n",
    "#                   min_param_tensor, max_param_tensor, min_water_t1t2_tensor, max_water_t1t2_tensor,\n",
    "#                   min_mt_param_fs_ksw, max_mt_param_fs_ksw,  min_amide_param_fs_ksw, max_amide_param_fs_ksw, nn_fn, step_size, gamma, net_name):\n",
    "#     nn_folder = os.path.dirname(nn_fn)  # Navigate up one directory level\n",
    "#     if not os.path.exists(nn_folder):\n",
    "#         os.makedirs(nn_folder)\n",
    "# \n",
    "#     # Initializing the reconstruction network\n",
    "#     reco_net = Network(sched_iter, add_iter=add_iter, n_hidden=2, n_neurons=300).to(device)\n",
    "# \n",
    "#     # Print amount of parameters\n",
    "#     print('Number of model parameters: ', sum(p.numel() for p in reco_net.parameters() if p.requires_grad))\n",
    "# \n",
    "#     # Setting optimizer\n",
    "#     optimizer = torch.optim.Adam(reco_net.parameters(), lr=learning_rate)\n",
    "#     scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "# \n",
    "#     # Storing current time\n",
    "#     t0 = time.time()\n",
    "#     # Get today's date\n",
    "#     today = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "#     writer = SummaryWriter(log_dir=f'runs/{net_name}')\n",
    "# \n",
    "#     loss_per_epoch = []\n",
    "#     val_loss_per_epoch = []\n",
    "#     patience_counter = 0\n",
    "#     min_loss = 100\n",
    "# \n",
    "#     reco_net.train()\n",
    "#     cur_val_loss = float('inf')\n",
    "# \n",
    "#     pbar = tqdm.tqdm(total=num_epochs)\n",
    "#     for epoch in range(num_epochs):\n",
    "#         # Cumulative loss\n",
    "#         cum_loss = 0\n",
    "#         counter = np.nan\n",
    "#         \n",
    "#         num_steps = len(train_loader)\n",
    "#         inner_pbar = tqdm.tqdm(total=num_steps)\n",
    "#         for counter, dict_params in enumerate(train_loader, 0):\n",
    "#             reco_net, cum_loss = train_step(device, noise_std, reco_net, optimizer, cum_loss, dict_params,\n",
    "#                                             min_param_tensor, max_param_tensor,\n",
    "#                                             min_water_t1t2_tensor, max_water_t1t2_tensor,\n",
    "#                                             min_mt_param_fs_ksw, max_mt_param_fs_ksw, min_amide_param_fs_ksw, max_amide_param_fs_ksw, \n",
    "#                                             writer, epoch, counter, num_steps)\n",
    "#             inner_pbar.set_description(f'Step: {counter+1}/{num_steps}')\n",
    "#             inner_pbar.update(1)\n",
    "#             \n",
    "#             del dict_params\n",
    "#             torch.cuda.empty_cache()\n",
    "#         inner_pbar.close()\n",
    "# \n",
    "#         # Average loss for this epoch\n",
    "#         loss_per_epoch.append(cum_loss / (counter + 1))\n",
    "#         \n",
    "#         # Validate the model\n",
    "#         val_loss = validate(reco_net, val_loader, device, min_param_tensor, max_param_tensor,\n",
    "#                             min_water_t1t2_tensor, max_water_t1t2_tensor,\n",
    "#                             min_mt_param_fs_ksw, max_mt_param_fs_ksw, min_amide_param_fs_ksw, max_amide_param_fs_ksw)\n",
    "#         val_loss_per_epoch.append(val_loss)\n",
    "#         \n",
    "#         writer.add_scalar(\"Loss/train\", loss_per_epoch[-1], epoch)\n",
    "#         writer.add_scalar(\"Loss/val\", val_loss, epoch)\n",
    "# \n",
    "#         pbar.set_description(f'Epoch: {epoch + 1}/{num_epochs}, Train Loss = {loss_per_epoch[-1]}, Val Loss = {val_loss_per_epoch[-1]}')\n",
    "#         pbar.update(1)\n",
    "# \n",
    "#         # Early stopping logic\n",
    "#         if (min_loss - val_loss_per_epoch[-1]) / min_loss > min_delta:\n",
    "#             min_loss = val_loss_per_epoch[-1]\n",
    "#             patience_counter = 0\n",
    "#         else:\n",
    "#             patience_counter += 1\n",
    "# \n",
    "#         if patience_counter > patience:\n",
    "#             print('Early stopping!')\n",
    "#             break\n",
    "#             \n",
    "#         # Scheduler step\n",
    "#         scheduler.step()\n",
    "# \n",
    "#         # Save model checkpoint when val loss gets better\n",
    "#         if val_loss <= cur_val_loss:\n",
    "#             print(f\"\\nSaved epoch {epoch} model\")\n",
    "#             torch.save({\n",
    "#                 'model_state_dict': reco_net.state_dict(),\n",
    "#                 'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                 'loss_per_epoch': loss_per_epoch,\n",
    "#                 'val_loss_per_epoch': val_loss_per_epoch,\n",
    "#                 'noise_std': noise_std,\n",
    "#                 'epoch': epoch\n",
    "#             }, nn_fn)\n",
    "#             \n",
    "#             torch.cuda.empty_cache()\n",
    "#             cur_val_loss = val_loss\n",
    "# \n",
    "#     pbar.close()\n",
    "#     print(f\"Training took {time.time() - t0:.2f} seconds\")\n",
    "# \n",
    "#     # # Save final model checkpoint\n",
    "#     # torch.save({\n",
    "#     #     'model_state_dict': reco_net.state_dict(),\n",
    "#     #     'optimizer_state_dict': optimizer.state_dict(),\n",
    "#     #     'loss_per_epoch': loss_per_epoch,\n",
    "#     #     'val_loss_per_epoch': val_loss_per_epoch,\n",
    "#     #     'noise_std': noise_std,\n",
    "#     # }, nn_fn)\n",
    "# \n",
    "#     writer.flush()\n",
    "#     writer.close()\n",
    "# \n",
    "#     # Test the model\n",
    "#     test_loss = test(reco_net, test_loader, device, min_param_tensor, max_param_tensor,\n",
    "#                      min_water_t1t2_tensor, max_water_t1t2_tensor,\n",
    "#                      min_mt_param_fs_ksw, max_mt_param_fs_ksw, min_amide_param_fs_ksw, max_amide_param_fs_ksw)\n",
    "#     print(f\"Test Loss: {test_loss}\")\n",
    "# \n",
    "#     return reco_net\n",
    "# \n",
    "# def train_step(device, noise_std, reco_net, optimizer, cum_loss, dict_params, min_param_tensor, max_param_tensor,\n",
    "#                min_water_t1t2_tensor, max_water_t1t2_tensor, min_mt_param_fs_ksw, max_mt_param_fs_ksw, min_amide_param_fs_ksw, max_amide_param_fs_ksw,\n",
    "#                writer, epoch, counter, num_steps):\n",
    "#     cur_fs, cur_ksw, cur_t1w, cur_t2w, cur_mt_fs, cur_mt_ksw, cur_amide_fs, cur_amide_ksw, cur_norm_sig = dict_params\n",
    "# \n",
    "#     target = torch.stack((cur_fs, cur_ksw), dim=1).to(device)\n",
    "#     input_water_t1t2 = torch.stack((cur_t1w, cur_t2w), dim=1).to(device)\n",
    "#     input_mt_fs_ksw = torch.stack((cur_mt_fs, cur_mt_ksw), dim=1).to(device)\n",
    "#     input_amide_fs_ksw = torch.stack((cur_amide_fs, cur_amide_ksw), dim=1).to(device)\n",
    "# \n",
    "#     # Normalizing the target and input_water_t1t2\n",
    "#     target = normalize_range(original_array=target, original_min=min_param_tensor,\n",
    "#                              original_max=max_param_tensor, new_min=0, new_max=1).to(device)\n",
    "# \n",
    "#     input_water_t1t2 = normalize_range(original_array=input_water_t1t2, original_min=min_water_t1t2_tensor,\n",
    "#                                        original_max=max_water_t1t2_tensor, new_min=0, new_max=1).to(device)\n",
    "# \n",
    "#     input_mt_fs_ksw = normalize_range(original_array=input_mt_fs_ksw, original_min=min_mt_param_fs_ksw,\n",
    "#                                       original_max=max_mt_param_fs_ksw, new_min=0, new_max=1).to(device)\n",
    "#     \n",
    "#     input_amide_fs_ksw = normalize_range(original_array=input_amide_fs_ksw, original_min=min_amide_param_fs_ksw,\n",
    "#                                   original_max=max_amide_param_fs_ksw, new_min=0, new_max=1).to(device)\n",
    "# \n",
    "#     # Adding noise to the input signals (trajectories)\n",
    "#     noised_sig = cur_norm_sig + torch.randn(cur_norm_sig.size()) * noise_std\n",
    "# \n",
    "#     # adding the mt_fs_ksw and t1, t2 as additional nn input\n",
    "#     noised_sig = torch.hstack((input_amide_fs_ksw, input_mt_fs_ksw, input_water_t1t2, noised_sig.to(device))).to(device)\n",
    "#     del input_water_t1t2, input_mt_fs_ksw, input_amide_fs_ksw\n",
    "# \n",
    "#     # Forward step\n",
    "#     prediction = reco_net(noised_sig.float())\n",
    "#     del noised_sig\n",
    "# \n",
    "#     # Batch loss (MSE)\n",
    "#     loss = torch.mean((prediction.float() - target.float()) ** 2)\n",
    "#     del target\n",
    "# \n",
    "#     # Backward step\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "# \n",
    "#     # Optimization step\n",
    "#     optimizer.step()\n",
    "# \n",
    "#     # Storing Cumulative loss\n",
    "#     cum_loss += loss.item()\n",
    "#     \n",
    "#     writer.add_scalar(\"Loss/train_step\", loss.item(), counter+epoch*num_steps)\n",
    "#     \n",
    "#     torch.cuda.empty_cache()\n",
    "#     \n",
    "#     return reco_net, cum_loss\n",
    "# \n",
    "# \n",
    "# def validate(reco_net, val_loader, device, min_param_tensor, max_param_tensor, min_water_t1t2_tensor, max_water_t1t2_tensor,\n",
    "#              min_mt_param_fs_ksw, max_mt_param_fs_ksw, min_amide_param_fs_ksw, max_amide_param_fs_ksw):\n",
    "#     reco_net.eval()\n",
    "#     val_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for dict_params in val_loader:\n",
    "#             cur_fs, cur_ksw, cur_t1w, cur_t2w, cur_mt_fs, cur_mt_ksw, cur_amide_fs, cur_amide_ksw, cur_norm_sig = dict_params\n",
    "# \n",
    "#             target = torch.stack((cur_fs, cur_ksw), dim=1).to(device)\n",
    "#             input_water_t1t2 = torch.stack((cur_t1w, cur_t2w), dim=1).to(device)\n",
    "#             input_mt_fs_ksw = torch.stack((cur_mt_fs, cur_mt_ksw), dim=1).to(device)\n",
    "#             input_amide_fs_ksw = torch.stack((cur_amide_fs, cur_amide_ksw), dim=1).to(device)\n",
    "# \n",
    "#             target = normalize_range(original_array=target, original_min=min_param_tensor,\n",
    "#                                      original_max=max_param_tensor, new_min=0, new_max=1).to(device)\n",
    "#             input_water_t1t2 = normalize_range(original_array=input_water_t1t2, original_min=min_water_t1t2_tensor,\n",
    "#                                                original_max=max_water_t1t2_tensor, new_min=0, new_max=1).to(device)\n",
    "#             input_mt_fs_ksw = normalize_range(original_array=input_mt_fs_ksw, original_min=min_mt_param_fs_ksw,\n",
    "#                                               original_max=max_mt_param_fs_ksw, new_min=0, new_max=1).to(device)\n",
    "#             input_amide_fs_ksw = normalize_range(original_array=input_amide_fs_ksw, original_min=min_amide_param_fs_ksw,\n",
    "#                               original_max=max_amide_param_fs_ksw, new_min=0, new_max=1).to(device)\n",
    "# \n",
    "#             noised_sig = cur_norm_sig\n",
    "# \n",
    "#             noised_sig = torch.hstack((input_amide_fs_ksw, input_mt_fs_ksw, input_water_t1t2, noised_sig.to(device))).to(device)\n",
    "# \n",
    "#             prediction = reco_net(noised_sig.float())\n",
    "# \n",
    "#             loss = torch.mean((prediction.float() - target.float()) ** 2)\n",
    "# \n",
    "#             val_loss += loss.item()\n",
    "# \n",
    "#     return val_loss / len(val_loader)\n",
    "# \n",
    "# \n",
    "# def test(reco_net, test_loader, device, min_param_tensor, max_param_tensor, min_water_t1t2_tensor, max_water_t1t2_tensor,\n",
    "#          min_mt_param_fs_ksw, max_mt_param_fs_ksw, min_amide_param_fs_ksw, max_amide_param_fs_ksw):\n",
    "#     reco_net.eval()\n",
    "#     test_loss = 0\n",
    "#     with torch.no_grad():\n",
    "#         for dict_params in test_loader:\n",
    "#             cur_fs, cur_ksw, cur_t1w, cur_t2w, cur_mt_fs, cur_mt_ksw, cur_amide_fs, cur_amide_ksw, cur_norm_sig = dict_params\n",
    "# \n",
    "#             target = torch.stack((cur_fs, cur_ksw), dim=1).to(device)\n",
    "#             input_water_t1t2 = torch.stack((cur_t1w, cur_t2w), dim=1).to(device)\n",
    "#             input_mt_fs_ksw = torch.stack((cur_mt_fs, cur_mt_ksw), dim=1).to(device)\n",
    "#             input_amide_fs_ksw = torch.stack((cur_amide_fs, cur_amide_ksw), dim=1).to(device)\n",
    "# \n",
    "#             target = normalize_range(original_array=target, original_min=min_param_tensor,\n",
    "#                                      original_max=max_param_tensor, new_min=0, new_max=1).to(device)\n",
    "#             input_water_t1t2 = normalize_range(original_array=input_water_t1t2, original_min=min_water_t1t2_tensor,\n",
    "#                                                original_max=max_water_t1t2_tensor, new_min=0, new_max=1).to(device)\n",
    "#             input_mt_fs_ksw = normalize_range(original_array=input_mt_fs_ksw, original_min=min_mt_param_fs_ksw,\n",
    "#                                               original_max=max_mt_param_fs_ksw, new_min=0, new_max=1).to(device)\n",
    "#             input_amide_fs_ksw = normalize_range(original_array=input_amide_fs_ksw, original_min=min_amide_param_fs_ksw,\n",
    "#                   original_max=max_amide_param_fs_ksw, new_min=0, new_max=1).to(device)\n",
    "# \n",
    "#             noised_sig = cur_norm_sig\n",
    "# \n",
    "#             noised_sig = torch.hstack((input_amide_fs_ksw, input_mt_fs_ksw, input_water_t1t2, noised_sig.to(device))).to(device)\n",
    "# \n",
    "#             prediction = reco_net(noised_sig.float())\n",
    "# \n",
    "#             loss = torch.mean((prediction.float() - target.float()) ** 2)\n",
    "# \n",
    "#             test_loss += loss.item()\n",
    "# \n",
    "#     return test_loss / len(test_loader)\n",
    "# \n",
    "# \n",
    "# def define_min_max(memmap_fn, sched_iter, add_iter, device):\n",
    "#     num_columns = sched_iter + add_iter + 2\n",
    "#     memmap_array = np.memmap(memmap_fn, dtype=np.float64, mode='r')\n",
    "#     num_rows = memmap_array.size // num_columns  # Calculate the number of rows\n",
    "#     memmap_array.shape = (num_rows, num_columns)  # [#, 30+6]\n",
    "# \n",
    "#     min_fs = np.min(memmap_array[:, 4])  # uncomment if non-zero minimum limit is required\n",
    "#     min_ksw = np.min(memmap_array[:, 5].transpose().astype(float))  # uncomment if non-zero minimum limit needed\n",
    "#     max_fs = np.max(memmap_array[:, 4])\n",
    "#     max_ksw = np.max(memmap_array[:, 5].transpose().astype(float))\n",
    "# \n",
    "#     min_t1w = np.min(memmap_array[:, 2])\n",
    "#     min_t2w = np.min(memmap_array[:, 3].transpose().astype(float))\n",
    "#     max_t1w = np.max(memmap_array[:, 2])\n",
    "#     max_t2w = np.max(memmap_array[:, 3].transpose().astype(float))\n",
    "# \n",
    "#     min_mt_fs = np.min(memmap_array[:, 6])\n",
    "#     min_mt_ksw = np.min(memmap_array[:, 7].transpose().astype(float))\n",
    "#     max_mt_fs = np.max(memmap_array[:, 6])\n",
    "#     max_mt_ksw = np.max(memmap_array[:, 7].transpose().astype(float))\n",
    "# \n",
    "#     min_amine_fs = np.min(memmap_array[:, 0])\n",
    "#     min_amine_ksw = np.min(memmap_array[:, 1].transpose().astype(float))\n",
    "#     max_amine_fs = np.max(memmap_array[:, 0])\n",
    "#     max_amine_ksw = np.max(memmap_array[:, 1].transpose().astype(float))\n",
    "# \n",
    "#     min_param_tensor = torch.tensor(np.hstack((min_fs, min_ksw)), requires_grad=False).to(device)  # can be switched to  min_fs, min_ksw\n",
    "#     max_param_tensor = torch.tensor(np.hstack((max_fs, max_ksw)), requires_grad=False).to(device)\n",
    "# \n",
    "#     min_water_t1t2_tensor = torch.tensor(np.hstack((min_t1w, min_t2w)), requires_grad=False).to(device)\n",
    "#     max_water_t1t2_tensor = torch.tensor(np.hstack((max_t1w, max_t2w)), requires_grad=False).to(device)\n",
    "# \n",
    "#     min_mt_param_tensor = torch.tensor(np.hstack((min_mt_fs, min_mt_ksw)), requires_grad=False).to(device)  # can be switched to  min_fs, min_ksw\n",
    "#     max_mt_param_tensor = torch.tensor(np.hstack((max_mt_fs, max_mt_ksw)), requires_grad=False).to(device)  # can be switched to  min_fs, min_ksw\n",
    "# \n",
    "#     min_amine_param_tensor = torch.tensor(np.hstack((min_amine_fs, min_amine_ksw)), requires_grad=False).to(device)  # can be switched to  min_fs, min_ksw\n",
    "#     max_amine_param_tensor = torch.tensor(np.hstack((max_amine_fs, max_amine_ksw)),\n",
    "#                                        requires_grad=False).to(device)  # can be switched to  min_fs, min_ksw\n",
    "# \n",
    "#     return (min_param_tensor, max_param_tensor, min_water_t1t2_tensor, max_water_t1t2_tensor,\n",
    "#             min_mt_param_tensor, max_mt_param_tensor, min_amine_param_tensor, max_amine_param_tensor)\n",
    "# \n",
    "# \n",
    "# if __name__ == '__main__':\n",
    "#     if platform.system() == 'Windows':\n",
    "#         multiprocessing.set_start_method('spawn', force=True)\n",
    "#     # os.chdir(os.path.dirname(os.path.realpath(__file__)))\n",
    "#     set_seed(2024)\n",
    "# \n",
    "#     main()\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "228da03a224ce80a"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "416b575517c8eb79"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
