{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import platform\n",
    "import torch\n",
    "import numpy as np\n",
    "import datetime\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "import time\n",
    "import os\n",
    "# import tqdm\n",
    "import tqdm.notebook as tqdm\n",
    "\n",
    "from utils.normalization import normalize_range, un_normalize_range\n",
    "from utils.seed import set_seed\n",
    "\n",
    "from sequential_nn.dataset import GluMemDataset\n",
    "from sequential_nn.model import Network\n",
    "from sequential_nn.multi_dict import pkl_2_dat, define_min_max\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:25:56.150185600Z",
     "start_time": "2024-07-17T10:25:55.750150300Z"
    }
   },
   "id": "ff0f2e0f1adc82dc",
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir=./runs --port 6007"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:25:56.400436100Z",
     "start_time": "2024-07-17T10:25:56.151833600Z"
    }
   },
   "id": "f37ee3b0d89ea3c1",
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 2024\n",
      "Using device: cuda\n",
      "There are 73785600 entries in the training dictionary\n",
      "Number of model parameters:  192902\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'batch_size' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[7], line 238\u001B[0m\n\u001B[0;32m    235\u001B[0m \u001B[38;5;66;03m# os.chdir(os.path.dirname(os.path.realpath(__file__)))\u001B[39;00m\n\u001B[0;32m    236\u001B[0m set_seed(\u001B[38;5;241m2024\u001B[39m)\n\u001B[1;32m--> 238\u001B[0m \u001B[43mmain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[7], line 72\u001B[0m, in \u001B[0;36mmain\u001B[1;34m()\u001B[0m\n\u001B[0;32m     65\u001B[0m train_dataset \u001B[38;5;241m=\u001B[39m GluMemDataset(memmap_fn, sched_iter, add_iter)\n\u001B[0;32m     67\u001B[0m train_loader \u001B[38;5;241m=\u001B[39m DataLoader(dataset\u001B[38;5;241m=\u001B[39mtrain_dataset,\n\u001B[0;32m     68\u001B[0m                           batch_size\u001B[38;5;241m=\u001B[39mbatch_size,\n\u001B[0;32m     69\u001B[0m                           shuffle\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m     70\u001B[0m                           num_workers\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n\u001B[1;32m---> 72\u001B[0m \u001B[43mtrain_network\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtrain_loader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msched_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43madd_iter\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdict_name\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlearning_rate\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_epochs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnoise_std\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpatience\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     73\u001B[0m \u001B[43m              \u001B[49m\u001B[43mmin_delta\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmin_param_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_param_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmin_water_t1t2_tensor\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     74\u001B[0m \u001B[43m              \u001B[49m\u001B[43mmax_water_t1t2_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmin_mt_param_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmax_mt_param_tensor\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnn_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstep_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgamma\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[7], line 104\u001B[0m, in \u001B[0;36mtrain_network\u001B[1;34m(train_loader, device, sched_iter, add_iter, dict_name, learning_rate, num_epochs, noise_std, patience, min_delta, min_param_tensor, max_param_tensor, min_water_t1t2_tensor, max_water_t1t2_tensor, min_mt_param_fs_ksw, max_mt_param_fs_ksw, nn_fn, step_size, gamma)\u001B[0m\n\u001B[0;32m    102\u001B[0m \u001B[38;5;66;03m# Get today's date\u001B[39;00m\n\u001B[0;32m    103\u001B[0m today \u001B[38;5;241m=\u001B[39m datetime\u001B[38;5;241m.\u001B[39mdatetime\u001B[38;5;241m.\u001B[39mnow()\u001B[38;5;241m.\u001B[39mstrftime(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mY-\u001B[39m\u001B[38;5;124m%\u001B[39m\u001B[38;5;124mm-\u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m--> 104\u001B[0m writer \u001B[38;5;241m=\u001B[39m SummaryWriter(log_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mruns/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdict_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_dict_noise_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mnoise_std\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_lr_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mlearning_rate\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstep_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mgamma\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mbatch_size\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtoday\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m    106\u001B[0m loss_per_epoch \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m    107\u001B[0m patience_counter \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "\u001B[1;31mNameError\u001B[0m: name 'batch_size' is not defined"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    torch.multiprocessing.freeze_support()\n",
    "    # i fucked up 'mouse_aacid_k_regular_nn_noise_005'\n",
    "    dict_name_category = 'dense'  # 1300, dense\n",
    "    dict_name = 'dense_1300'  # mouse_20, dense_1300, \n",
    "    fp_prtcl_name = '107a'\n",
    "\n",
    "    # Schedule iterations\n",
    "    # number of raw images in the CEST-MRF acquisition schedule\n",
    "    sched_iter = 30\n",
    "    add_iter = 4\n",
    "\n",
    "    # Training properties\n",
    "    learning_rate = 2e-4\n",
    "    step_size = 500\n",
    "    gamma = 0.8\n",
    "    batch_size = 2048\n",
    "    num_epochs = 1  # 150\n",
    "    noise_std = 1e-2  # noise level for training, 1e-2\n",
    "\n",
    "    min_delta = 0.05  # minimum absolute change in the loss function\n",
    "    patience = np.inf\n",
    "\n",
    "    current_dir = os.getcwd()  # Get the current directory\n",
    "    parent_dir = os.path.dirname(current_dir)  # Navigate up one directory level\n",
    "    glu_dict_folder_fn = os.path.join(parent_dir, 'data', 'exp', 'mt_glu_dicts', dict_name_category, 'glu', dict_name,\n",
    "                                      fp_prtcl_name)  # dict folder directory\n",
    "    memmap_fn = os.path.join(glu_dict_folder_fn, 'dict.dat')\n",
    "\n",
    "    if not os.path.exists(memmap_fn):\n",
    "        pkl_2_dat(glu_dict_folder_fn, sched_iter, add_iter)\n",
    "\n",
    "    net_name = f'noise_{noise_std}'\n",
    "    nn_fn = os.path.join(current_dir, 'mouse_nns', 'glu_mt_nns', dict_name_category, 'glu', dict_name, fp_prtcl_name,\n",
    "                         f'{net_name}.pt')  # nn directory\n",
    "\n",
    "    device = initialize_device()\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    (min_param_tensor, max_param_tensor,\n",
    "     min_water_t1t2_tensor, max_water_t1t2_tensor,\n",
    "     min_mt_param_tensor, max_mt_param_tensor) = define_min_max(memmap_fn, sched_iter, add_iter, device)\n",
    "\n",
    "    # Convert tensors to numpy arrays\n",
    "    min_param_array = min_param_tensor.cpu().numpy()\n",
    "    max_param_array = max_param_tensor.cpu().numpy()\n",
    "    min_water_t1t2_array = min_water_t1t2_tensor.cpu().numpy()\n",
    "    max_water_t1t2_array = max_water_t1t2_tensor.cpu().numpy()\n",
    "    min_mt_param_array = min_mt_param_tensor.cpu().numpy()\n",
    "    max_mt_param_array = max_mt_param_tensor.cpu().numpy()\n",
    "    \n",
    "    if not os.path.exists(os.path.dirname(nn_fn)):\n",
    "        os.makedirs(os.path.dirname(nn_fn))\n",
    "    # Save all arrays to a single .npz file\n",
    "    np.savez(os.path.join(os.path.dirname(nn_fn),'min_max_values.npz'),\n",
    "             min_param=min_param_array,\n",
    "             max_param=max_param_array,\n",
    "             min_water_t1t2=min_water_t1t2_array,\n",
    "             max_water_t1t2=max_water_t1t2_array,\n",
    "             min_mt_param=min_mt_param_array,\n",
    "             max_mt_param=max_mt_param_array)\n",
    "\n",
    "    # Loading the training dataset\n",
    "    # train_dataset = GluMemDataset(memmap_fn, sched_iter, add_iter, chunk_size=10000000)\n",
    "    train_dataset = GluMemDataset(memmap_fn, sched_iter, add_iter)\n",
    "\n",
    "    train_loader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              num_workers=0)\n",
    "\n",
    "    train_network(train_loader, device, sched_iter, add_iter, dict_name, learning_rate, num_epochs, noise_std, patience,\n",
    "                  min_delta, min_param_tensor, max_param_tensor, min_water_t1t2_tensor,\n",
    "                  max_water_t1t2_tensor, min_mt_param_tensor, max_mt_param_tensor, nn_fn, step_size, gamma)\n",
    "\n",
    "\n",
    "# Function to initialize device\n",
    "def initialize_device():\n",
    "    return 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "\n",
    "# Function to train the network\n",
    "def train_network(train_loader, device, sched_iter, add_iter, dict_name, learning_rate, num_epochs, noise_std, patience, min_delta,\n",
    "                  min_param_tensor, max_param_tensor, min_water_t1t2_tensor, max_water_t1t2_tensor,\n",
    "                  min_mt_param_fs_ksw, max_mt_param_fs_ksw, nn_fn, step_size, gamma):\n",
    "    nn_folder = os.path.dirname(nn_fn)  # Navigate up one directory level\n",
    "    if not os.path.exists(nn_folder):\n",
    "        os.makedirs(nn_folder)\n",
    "\n",
    "    # Initializing the reconstruction network\n",
    "    reco_net = Network(sched_iter, add_iter=add_iter, n_hidden=2, n_neurons=300).to(device)\n",
    "\n",
    "    # Print amount of parameters\n",
    "    print('Number of model parameters: ', sum(p.numel() for p in reco_net.parameters() if p.requires_grad))\n",
    "\n",
    "    # Setting optimizer\n",
    "    optimizer = torch.optim.Adam(reco_net.parameters(), lr=learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    # Storing current time\n",
    "    t0 = time.time()\n",
    "    # Get today's date\n",
    "    today = datetime.datetime.now().strftime('%Y-%m-%d')\n",
    "    writer = SummaryWriter(log_dir=f'runs/{dict_name}_dict_noise_{noise_std}_lr_{learning_rate}_{step_size}_{gamma}_{batch_size}_{today}')\n",
    "\n",
    "    loss_per_epoch = []\n",
    "    patience_counter = 0\n",
    "    min_loss = 100\n",
    "\n",
    "    reco_net.train()\n",
    "\n",
    "    pbar = tqdm.tqdm(total=num_epochs)\n",
    "    for epoch in range(num_epochs):\n",
    "        # Cumulative loss\n",
    "        cum_loss = 0\n",
    "        counter = np.nan\n",
    "        \n",
    "        num_steps = len(train_loader)\n",
    "        inner_pbar = tqdm.tqdm(total=num_steps)\n",
    "        for counter, dict_params in enumerate(train_loader, 0):\n",
    "            reco_net, cum_loss = train_step(device, noise_std, reco_net, optimizer, cum_loss, dict_params,\n",
    "                                            min_param_tensor, max_param_tensor,\n",
    "                                            min_water_t1t2_tensor, max_water_t1t2_tensor,\n",
    "                                            min_mt_param_fs_ksw, max_mt_param_fs_ksw, writer, epoch, counter)\n",
    "            inner_pbar.set_description(f'Step: {counter+1}/{num_steps}')\n",
    "            inner_pbar.update(1)\n",
    "            \n",
    "            del dict_params\n",
    "            torch.cuda.empty_cache()\n",
    "        inner_pbar.close()\n",
    "\n",
    "        # Average loss for this epoch\n",
    "        loss_per_epoch.append(cum_loss / (counter + 1))\n",
    "        # writer.add_scalar(\"Loss/train\", loss_per_epoch, epoch)\n",
    "\n",
    "        pbar.set_description(f'Epoch: {epoch + 1}/{num_epochs}, Loss = {loss_per_epoch[-1]}')\n",
    "        pbar.update(1)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if (min_loss - loss_per_epoch[-1]) / min_loss > min_delta:\n",
    "            min_loss = loss_per_epoch[-1]\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter > patience:\n",
    "            print('Early stopping!')\n",
    "            break\n",
    "\n",
    "        # Save model checkpoint every 25 epochs (excluding epoch 0)\n",
    "        if epoch % 10 == 0 and epoch != 0:\n",
    "            print(f\"\\nSaved epoch {epoch} model\")\n",
    "            torch.save({\n",
    "                'model_state_dict': reco_net.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss_per_epoch': loss_per_epoch,\n",
    "                'noise_std': noise_std,\n",
    "                'epoch': epoch\n",
    "            }, nn_fn)\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    pbar.close()\n",
    "    print(f\"Training took {time.time() - t0:.2f} seconds\")\n",
    "\n",
    "    # Save final model checkpoint\n",
    "    torch.save({\n",
    "        'model_state_dict': reco_net.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss_per_epoch': loss_per_epoch,\n",
    "        'noise_std': noise_std,\n",
    "    }, nn_fn)\n",
    "\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "\n",
    "    return reco_net\n",
    "\n",
    "\n",
    "def train_step(device, noise_std, reco_net, optimizer, cum_loss, dict_params, min_param_tensor, max_param_tensor,\n",
    "               min_water_t1t2_tensor, max_water_t1t2_tensor, min_mt_param_fs_ksw, max_mt_param_fs_ksw, writer, epoch, counter):\n",
    "    cur_fs, cur_ksw, cur_t1w, cur_t2w, cur_mt_fs, cur_mt_ksw, cur_norm_sig = dict_params\n",
    "\n",
    "    target = torch.stack((cur_fs, cur_ksw), dim=1).to(device)\n",
    "    input_water_t1t2 = torch.stack((cur_t1w, cur_t2w), dim=1).to(device)\n",
    "    input_mt_fs_ksw = torch.stack((cur_mt_fs, cur_mt_ksw), dim=1).to(device)\n",
    "\n",
    "    # Normalizing the target and input_water_t1t2\n",
    "    target = normalize_range(original_array=target, original_min=min_param_tensor,\n",
    "                             original_max=max_param_tensor, new_min=0, new_max=1).to(device)\n",
    "\n",
    "    input_water_t1t2 = normalize_range(original_array=input_water_t1t2, original_min=min_water_t1t2_tensor,\n",
    "                                       original_max=max_water_t1t2_tensor, new_min=0, new_max=1).to(device)\n",
    "\n",
    "    input_mt_fs_ksw = normalize_range(original_array=input_mt_fs_ksw, original_min=min_mt_param_fs_ksw,\n",
    "                                      original_max=max_mt_param_fs_ksw, new_min=0, new_max=1).to(device)\n",
    "\n",
    "    # Adding noise to the input signals (trajectories)\n",
    "    noised_sig = cur_norm_sig + torch.randn(cur_norm_sig.size()) * noise_std\n",
    "\n",
    "    # noised_sig = noised_sig / torch.linalg.norm(noised_sig, dim=1, ord=2, keepdim=True)\n",
    "\n",
    "    # adding the mt_fs_ksw and t1, t2 as additional nn input\n",
    "    noised_sig = torch.hstack((input_mt_fs_ksw, input_water_t1t2, noised_sig.to(device))).to(device)\n",
    "    del input_water_t1t2, input_mt_fs_ksw\n",
    "\n",
    "    # Forward step\n",
    "    prediction = reco_net(noised_sig.float())\n",
    "    del noised_sig\n",
    "\n",
    "    # Batch loss (MSE)\n",
    "    loss = torch.mean((prediction.float() - target.float()) ** 2)\n",
    "    del target\n",
    "\n",
    "    # Backward step\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "\n",
    "    # Optimization step\n",
    "    optimizer.step()\n",
    "\n",
    "    # Storing Cumulative loss\n",
    "    cum_loss += loss.item()\n",
    "    \n",
    "    writer.add_scalar(\"Loss/train\", loss.item(), counter)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return reco_net, cum_loss\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    if platform.system() == 'Windows':\n",
    "        multiprocessing.set_start_method('spawn', force=True)\n",
    "    # os.chdir(os.path.dirname(os.path.realpath(__file__)))\n",
    "    set_seed(2024)\n",
    "\n",
    "    main()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:34:36.600251600Z",
     "start_time": "2024-07-17T10:31:34.317286600Z"
    }
   },
   "id": "d0eb0c85a326f83f",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-17T10:31:07.894809800Z",
     "start_time": "2024-07-17T10:31:07.892774200Z"
    }
   },
   "id": "3f29685008ce0533",
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
