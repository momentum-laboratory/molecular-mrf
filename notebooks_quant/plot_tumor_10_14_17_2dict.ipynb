{
 "cells": [
  {
   "cell_type": "code",
   "id": "51f80d20073c3aaf",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T11:54:32.388737Z",
     "start_time": "2024-07-31T11:54:31.290569Z"
    }
   },
   "source": [
    "import os\n",
    "import time\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "from skimage.measure import regionprops\n",
    "\n",
    "from my_funcs.plot_functions import t1_t2_pixel_reader\n",
    "\n",
    "from my_funcs.cest_functions import bruker_dataset_creator, dicom_data_arranger, m0_normalizer\n",
    "from my_funcs.mask_functions import mask_processor, tumor_masks_processor, tumor_contra_masks_processor, contour_finder\n",
    "\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io as pio\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from sequential_nn.model import Network\n",
    "from utils.normalization import normalize_range, un_normalize_range"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Subject choice (0/1/2) #"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "eb6575eb6529a644"
  },
  {
   "cell_type": "code",
   "source": [
    "subject_i = 2  # 0, 1, 2\n",
    "save_name = 'mouse_june_tumor'\n",
    "fp_prtcl_name = '107a'  # to be looped later with all options # 107a , 51_glu\n",
    "M0_flag = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T11:54:32.391446Z",
     "start_time": "2024-07-31T11:54:32.389544Z"
    }
   },
   "id": "bf540ba48ab9979e",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "source": [
    "## subject data ##"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8916f21a35b03bb6"
  },
  {
   "cell_type": "code",
   "source": [
    "from my_funcs.mouse_data_dicts import subject_dicts_june_tumor_10, subject_dicts_june_tumor_14, subject_dicts_june_tumor_17\n",
    "\n",
    "# Root stats:\n",
    "general_fn = os.path.abspath(os.curdir)\n",
    "current_dir = os.getcwd()  # Get the current directory\n",
    "parent_dir = os.path.dirname(current_dir)  # Navigate up one directory level\n",
    "\n",
    "# Subject data:\n",
    "txt_file_name = 'labarchive_notes.txt'\n",
    "f_const = 3 / 110000"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T11:54:32.399107Z",
     "start_time": "2024-07-31T11:54:32.392314Z"
    }
   },
   "id": "50f3533a22729ef1",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Choose nn ##"
   ],
   "id": "939a5ed3f1841c94"
  },
  {
   "cell_type": "code",
   "source": [
    "# Use GPU if available (otherwise use CPU)\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "    print('GPU found and will be used')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('GPU was not found. Using CPU')\n",
    "\n",
    "sched_iter = 30\n",
    "dtype = torch.DoubleTensor"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T11:54:32.456449Z",
     "start_time": "2024-07-31T11:54:32.399606Z"
    }
   },
   "id": "fc3d44dfea5c8c16",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU found and will be used\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "source": [
    "# Choose dict and model:\n",
    "dict_name_category = 'glu_amide_lim'  # high_glu_conc_500, \n",
    "fp_prtcl_name = '107a'\n",
    "\n",
    "# MT:\n",
    "# Dict choice:\n",
    "mt_dict_fn = os.path.join(parent_dir, 'data', 'exp', 'mt_amide_glu_dicts', dict_name_category, 'mt', 'MT52', 'dict.pkl')  # dict folder directory\n",
    "if M0_flag:\n",
    "    mt_nn_fn = os.path.join(current_dir, 'mouse_nns', 'glu_amide_mt_nns', dict_name_category, 'mt', 'MT52', 'M0_noise_0.01.pt')  # nn directory\n",
    "else:\n",
    "    mt_nn_fn = os.path.join(current_dir, 'mouse_nns', 'glu_amide_mt_nns', dict_name_category, 'mt', 'MT52', 'noise_0.01.pt')  # nn directory\n",
    "\n",
    "# Glu-Amide:\n",
    "# Dict choice:\n",
    "ga_dict_folder_fn = os.path.join(parent_dir, 'data', 'exp', 'mt_amide_glu_dicts', dict_name_category, '2dict')  # dict folder directory\n",
    "# NN choice:\n",
    "ga_net_name = 'glu_amide_lim_2dict_noise_0.002_lr_0.0001_1024'\n",
    "ga_nn_fn = os.path.join(current_dir, 'mouse_nns', 'glu_amide_mt_nns', dict_name_category, '2dict', f'{ga_net_name}.pt')  # nn directory\n",
    "\n",
    "print(f'MT dict path: {mt_dict_fn}')\n",
    "print(f'MT nn path: {mt_nn_fn}')\n",
    "print(f'\\nGlu-Amide dict path: {ga_dict_folder_fn}')\n",
    "print(f'Glu-Amide nn path: {ga_nn_fn}')"
   ],
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T11:54:32.462029Z",
     "start_time": "2024-07-31T11:54:32.456999Z"
    }
   },
   "id": "546ef00d01a582a1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MT dict path: /home/hagar/mrf_final/data/exp/mt_amide_glu_dicts/glu_amide_lim/mt/MT52/dict.pkl\n",
      "MT nn path: /home/hagar/mrf_final/molecular-mrf/mouse_nns/glu_amide_mt_nns/glu_amide_lim/mt/MT52/noise_0.01.pt\n",
      "\n",
      "Glu-Amide dict path: /home/hagar/mrf_final/data/exp/mt_amide_glu_dicts/glu_amide_lim/2dict\n",
      "Glu-Amide nn path: /home/hagar/mrf_final/molecular-mrf/mouse_nns/glu_amide_mt_nns/glu_amide_lim/2dict/glu_amide_lim_2dict_noise_0.002_lr_0.0001_1024.pt\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# create image path if needed #"
   ],
   "id": "adbfe3a861827ca9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T11:54:32.469147Z",
     "start_time": "2024-07-31T11:54:32.462538Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def image_path_creator(save_name, phantom_choice, dict_name_category, fp_prtcl_name, glu_net_name):\n",
    "    subject_image_path = f'images/{save_name}/subject_{phantom_choice}'\n",
    "    if not os.path.exists(subject_image_path):\n",
    "        os.makedirs(subject_image_path)\n",
    "    \n",
    "    subject_mt_nn_image_path = os.path.join(subject_image_path, dict_name_category, 'mt')\n",
    "    if not os.path.exists(subject_mt_nn_image_path):\n",
    "        os.makedirs(subject_mt_nn_image_path) \n",
    "    # subject_amide_nn_image_path = os.path.join(subject_image_path, dict_name_category, 'amide')\n",
    "    # if not os.path.exists(subject_amide_nn_image_path):\n",
    "    #     os.makedirs(subject_amide_nn_image_path)\n",
    "    # \n",
    "    # subject_glu_nn_image_path = os.path.join(subject_image_path, dict_name_category, 'glu', fp_prtcl_name, glu_net_name)\n",
    "    # if not os.path.exists(subject_glu_nn_image_path):\n",
    "    #     os.makedirs(subject_glu_nn_image_path)\n",
    "        \n",
    "    subject_glu_amide_nn_image_path = os.path.join(subject_image_path, dict_name_category, '2dict', glu_net_name)\n",
    "    if not os.path.exists(subject_glu_amide_nn_image_path):\n",
    "        os.makedirs(subject_glu_amide_nn_image_path)\n",
    "        \n",
    "    return subject_image_path, subject_mt_nn_image_path, subject_glu_amide_nn_image_path"
   ],
   "id": "b646b6bc2751533f",
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data preparation ##"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4a9c851189c461c6"
  },
  {
   "cell_type": "code",
   "source": [
    "# Loading the mt acquired data\n",
    "def load_mrf(prtcl_name, glu_mouse_fn, txt_file_name, sched_iter, dtype, device, M0_flag=False):\n",
    "    fn, mouse_mrf_files_fn, bruker_dataset = bruker_dataset_creator(glu_mouse_fn, txt_file_name, prtcl_name)\n",
    "    \n",
    "    if M0_flag:\n",
    "        acquired_data = dicom_data_arranger(bruker_dataset, fn).astype(np.float)[1:,:,:]\n",
    "        acquired_data = m0_normalizer(acquired_data)\n",
    "    else:\n",
    "        acquired_data = dicom_data_arranger(bruker_dataset, fn).astype(np.float)[2:,:,:]\n",
    "    \n",
    "    [_, c_acq_data, w_acq_data] = np.shape(acquired_data)  # mask here\n",
    "    \n",
    "    # Reshaping the acquired data to the shape expected by the NN (e.g. 30 x ... )\n",
    "    acquired_data = np.reshape(acquired_data, (sched_iter, c_acq_data * w_acq_data), order='F')\n",
    "    \n",
    "    # 2-norm normalization of the dictionary signals\n",
    "    if M0_flag == False:\n",
    "        acquired_data = acquired_data / np.sqrt(np.sum(acquired_data ** 2, axis=0))\n",
    "    \n",
    "    # Transposing for compatibility with the NN - now each row is a trajectory\n",
    "    acquired_data = acquired_data.T\n",
    "\n",
    "    # Converting to tensor\n",
    "    acquired_data = Variable(torch.from_numpy(acquired_data).type(dtype), requires_grad=False).to(device)\n",
    "    \n",
    "    return acquired_data\n",
    "\n",
    "def data_preparation(subject_dict, glu_mouse_fn, txt_file_name, dtype, device, sched_iter, M0_flag):\n",
    "    acquired_data_52 = load_mrf('MT52', glu_mouse_fn, txt_file_name, sched_iter, dtype, device, M0_flag)\n",
    "    acquired_data_51 = load_mrf('51_Amide', glu_mouse_fn, txt_file_name, sched_iter, dtype, device, M0_flag)\n",
    "    acquired_data_107a = load_mrf('107a', glu_mouse_fn, txt_file_name, sched_iter, dtype, device, M0_flag)\n",
    "    \n",
    "    # Loading the separately acquired water_t1t2-maps\n",
    "    t1 = t1_t2_pixel_reader(glu_phantom_fn=glu_mouse_fn, txt_file_name=txt_file_name, image_idx=3, t_type='t1',\n",
    "                                   image_file=4)\n",
    "    t2 = t1_t2_pixel_reader(glu_phantom_fn=glu_mouse_fn, txt_file_name=txt_file_name, image_idx=3, t_type='t2')\n",
    "    [c_acq_data, w_acq_data] = np.shape(t1)  # mask here\n",
    "    \n",
    "    shift_up, shift_right = subject_dict['t_shift']\n",
    "    t1 = np.roll(t1, shift=(shift_up, shift_right), axis=(0, 1))\n",
    "    t2 = np.roll(t2, shift=(shift_up, shift_right), axis=(0, 1))\n",
    "    \n",
    "    # masked_t1 = np.ma.masked_where(mask == 0, t1)\n",
    "    # masked_t2 = np.ma.masked_where(mask == 0, t2)\n",
    "    # \n",
    "    # print(f't1 min-max: [{np.min(masked_t1), np.max(masked_t1)}]')\n",
    "    # print(f't2 min-max: [{np.min(masked_t2), np.max(masked_t2)}]')\n",
    "    \n",
    "    # Reshaping the acquired data to the shape expected by the NN (e.g. 2 x ... )\n",
    "    acquired_map_t1w_orig = t1.astype(np.float32) / 1000\n",
    "    acquired_map_t2w_orig = t2.astype(np.float32) / 1000\n",
    "    \n",
    "    acquired_map_t1w = np.reshape(acquired_map_t1w_orig, (1, c_acq_data * w_acq_data), order='F').T\n",
    "    acquired_map_t1w = torch.from_numpy(acquired_map_t1w)\n",
    "    \n",
    "    acquired_map_t2w = np.reshape(acquired_map_t2w_orig, (1, c_acq_data * w_acq_data), order='F').T\n",
    "    acquired_map_t2w = torch.from_numpy(acquired_map_t2w)\n",
    "    \n",
    "    return c_acq_data, w_acq_data, acquired_data_52, acquired_data_51, acquired_data_107a, acquired_map_t1w, acquired_map_t2w"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T11:54:32.476238Z",
     "start_time": "2024-07-31T11:54:32.470287Z"
    }
   },
   "id": "3a268089b777790b",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. MT step ##"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4c09c5649c72610c"
  },
  {
   "cell_type": "code",
   "source": [
    "def mt_step(mt_nn_fn, acquired_map_t1w, acquired_map_t2w, acquired_data_52, c_acq_data, w_acq_data, sched_iter, device):\n",
    "    if sched_iter == 30:\n",
    "        # load min-max of dataset\n",
    "        mt_min_max_data = np.load(os.path.join(os.path.dirname(mt_nn_fn),'min_max_values.npz'))\n",
    "        # Access each array by its key\n",
    "        min_param_tensor_0 = torch.tensor(mt_min_max_data['min_param'])\n",
    "        max_param_tensor_0 = torch.tensor(mt_min_max_data['max_param'])\n",
    "        min_water_t1t2_tensor_0 = torch.tensor(mt_min_max_data['min_water_t1t2'])\n",
    "        max_water_t1t2_tensor_0 = torch.tensor(mt_min_max_data['max_water_t1t2'])\n",
    "    elif sched_iter == 31:\n",
    "        \"Please check min max parameters\"\n",
    "        min_param_tensor_0 = torch.tensor([float(0), float(0)])\n",
    "        max_param_tensor_0 = torch.tensor([float(30000/110000), float(100)])\n",
    "        min_water_t1t2_tensor_0 = torch.tensor([float(1.3), float(0.04)])\n",
    "        max_water_t1t2_tensor_0 = torch.tensor([float(2.3), float(0.11)])\n",
    "    \n",
    "    \n",
    "    # load model\n",
    "    mt_reco_net = Network(sched_iter, add_iter=2).to(device)\n",
    "    checkpoint = torch.load(mt_nn_fn, map_location=torch.device(device))\n",
    "    mt_reco_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    loss_per_epoch_mt = checkpoint['loss_per_epoch']\n",
    "    \n",
    "    # Normalizing according to dict water_t1t2 min and max values\n",
    "    input_water_t1t2_acquired = torch.hstack((acquired_map_t1w, acquired_map_t2w))\n",
    "    input_water_t1t2_acquired = normalize_range(original_array=input_water_t1t2_acquired,\n",
    "                                                original_min=min_water_t1t2_tensor_0,\n",
    "                                                original_max=max_water_t1t2_tensor_0, new_min=0, new_max=1).to(device)\n",
    "    \n",
    "    # adding the water_t1t2 input as two additional elements in the noised_sig vector\n",
    "    acquired_data = torch.hstack((input_water_t1t2_acquired, acquired_data_52)).to(device).float()\n",
    "    \n",
    "    # evaluate\n",
    "    mt_reco_net.eval()\n",
    "    t0 = time.time()\n",
    "    prediction = mt_reco_net(acquired_data.float())\n",
    "    print(f\"Prediction took {time.time() - t0:.5f} seconds\")\n",
    "    \n",
    "    # Un-normalizing to go back to physical units\n",
    "    prediction = un_normalize_range(prediction, original_min=min_param_tensor_0.to(device),\n",
    "                                    original_max=max_param_tensor_0.to(device), new_min=0, new_max=1)\n",
    "\n",
    "    quant_maps_mt = {}\n",
    "    \n",
    "    # Reshaping back to the image dimension\n",
    "    quant_maps_mt['fs'] = prediction.cpu().detach().numpy()[:, 0]\n",
    "    quant_maps_mt['fs'] = quant_maps_mt['fs'].T\n",
    "    quant_maps_mt['fs'] = np.reshape(quant_maps_mt['fs'], (c_acq_data, w_acq_data), order='F')\n",
    "    \n",
    "    quant_maps_mt['ksw'] = prediction.cpu().detach().numpy()[:, 1]\n",
    "    quant_maps_mt['ksw'] = quant_maps_mt['ksw'].T\n",
    "    quant_maps_mt['ksw'] = np.reshape(quant_maps_mt['ksw'], (c_acq_data, w_acq_data), order='F')\n",
    "    \n",
    "    input_mt_param = prediction.cpu().detach().numpy()\n",
    "    input_mt_param = torch.from_numpy(input_mt_param)\n",
    "    \n",
    "    return loss_per_epoch_mt, input_mt_param, quant_maps_mt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T11:54:32.483283Z",
     "start_time": "2024-07-31T11:54:32.476823Z"
    }
   },
   "id": "b328ef85dba424f8",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Glu-Amide step ##"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d70ba2e698e7bc24"
  },
  {
   "cell_type": "code",
   "source": [
    "def ga_step(ga_nn_fn, acquired_map_t1w, acquired_map_t2w, acquired_data_51, acquired_data_107, input_mt_param, c_acq_data, w_acq_data, sched_iter, device):\n",
    "    # load min-max of dataset\n",
    "    ga_min_max_data = np.load(os.path.join(os.path.dirname(ga_nn_fn),'min_max_values.npz'))\n",
    "    # Target:\n",
    "    min_glu_param_tensor_1 = torch.tensor(ga_min_max_data['min_param'])\n",
    "    max_glu_param_tensor_1 = torch.tensor(ga_min_max_data['max_param'])\n",
    "    min_amide_param_tensor_1 = torch.tensor(ga_min_max_data['min_amide_param'])\n",
    "    max_amide_param_tensor_1 = torch.tensor(ga_min_max_data['max_amide_param'])\n",
    "    # Input:\n",
    "    min_water_t1t2_tensor_1 = torch.tensor(ga_min_max_data['min_water_t1t2'])\n",
    "    max_water_t1t2_tensor_1 = torch.tensor(ga_min_max_data['max_water_t1t2'])\n",
    "    min_mt_param_tensor_1 = torch.tensor(ga_min_max_data['min_mt_param'])\n",
    "    max_mt_param_tensor_1 = torch.tensor(ga_min_max_data['max_mt_param'])\n",
    "    \n",
    "    # load model\n",
    "    reco_net = Network(sched_iter*2, add_iter=4, output_dim=4).to(device)\n",
    "    checkpoint = torch.load(ga_nn_fn, map_location=torch.device(device))\n",
    "    reco_net.load_state_dict(checkpoint['model_state_dict'])\n",
    "    loss_per_epoch = checkpoint['loss_per_epoch']\n",
    "    # noise_std = checkpoint['noise_std']\n",
    "    \n",
    "    # Normalizing according to dict water_t1t2 min and max values\n",
    "    input_water_t1t2_acquired = torch.hstack((acquired_map_t1w, acquired_map_t2w))\n",
    "    input_water_t1t2_acquired = normalize_range(original_array=input_water_t1t2_acquired,\n",
    "                                                original_min=min_water_t1t2_tensor_1,\n",
    "                                                original_max=max_water_t1t2_tensor_1, new_min=0, new_max=1).to(device)\n",
    "    \n",
    "    input_mt_param = normalize_range(original_array=input_mt_param,\n",
    "                                                original_min=min_mt_param_tensor_1,\n",
    "                                                original_max=max_mt_param_tensor_1, new_min=0, new_max=1).to(device)\n",
    "    \n",
    "    # concat input\n",
    "    acquired_data = torch.hstack((input_mt_param, input_water_t1t2_acquired, acquired_data_107, acquired_data_51)).to(device).float()\n",
    "    \n",
    "    # evaluate\n",
    "    reco_net.eval()\n",
    "    t0 = time.time()\n",
    "    prediction = reco_net(acquired_data.float())\n",
    "    print(f\"Prediction took {time.time() - t0:.5f} seconds\")\n",
    "    \n",
    "    # Un-normalizing to go back to physical units\n",
    "    prediction = un_normalize_range(prediction, original_min=torch.hstack((min_glu_param_tensor_1, min_amide_param_tensor_1)).to(device),\n",
    "                                    original_max=torch.hstack((max_glu_param_tensor_1, max_amide_param_tensor_1)).to(device), new_min=0, new_max=1)\n",
    "    \n",
    "    quant_maps_glu = {}\n",
    "    quant_maps_amide = {}\n",
    "    \n",
    "    # Reshaping back to the image dimension\n",
    "    quant_maps_glu['fs'] = prediction.cpu().detach().numpy()[:, 0]\n",
    "    quant_maps_glu['fs'] = quant_maps_glu['fs'].T\n",
    "    quant_maps_glu['fs'] = np.reshape(quant_maps_glu['fs'], (c_acq_data, w_acq_data), order='F')\n",
    "    \n",
    "    quant_maps_glu['ksw'] = prediction.cpu().detach().numpy()[:, 1]\n",
    "    quant_maps_glu['ksw'] = quant_maps_glu['ksw'].T\n",
    "    quant_maps_glu['ksw'] = np.reshape(quant_maps_glu['ksw'], (c_acq_data, w_acq_data), order='F')\n",
    "    \n",
    "    # Reshaping back to the image dimension\n",
    "    quant_maps_amide['fs'] = prediction.cpu().detach().numpy()[:, 2]\n",
    "    quant_maps_amide['fs'] = quant_maps_amide['fs'].T\n",
    "    quant_maps_amide['fs'] = np.reshape(quant_maps_amide['fs'], (c_acq_data, w_acq_data), order='F')\n",
    "    \n",
    "    quant_maps_amide['ksw'] = prediction.cpu().detach().numpy()[:, 3]\n",
    "    quant_maps_amide['ksw'] = quant_maps_amide['ksw'].T\n",
    "    quant_maps_amide['ksw'] = np.reshape(quant_maps_amide['ksw'], (c_acq_data, w_acq_data), order='F')\n",
    "    \n",
    "    return loss_per_epoch, quant_maps_glu, quant_maps_amide"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T11:54:32.490389Z",
     "start_time": "2024-07-31T11:54:32.483820Z"
    }
   },
   "id": "d04330843f7fa993",
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "source": [
    "# plotting #"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c4aae9fcffdd1d7f"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## custom color-maps ##"
   ],
   "id": "8daa87c415ef6e04"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-07-31T11:54:32.498783Z",
     "start_time": "2024-07-31T11:54:32.490907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create custom Viridis colormap with black for 0 values\n",
    "custom_viridis = np.array(plotly.colors.sequential.Viridis)\n",
    "custom_viridis[0] = '#000000'  # Set black for 0 values\n",
    "\n",
    "custom_magma = np.array(plotly.colors.sequential.Magma)\n",
    "custom_magma[0] = '#000000'  # Set black for 0 values\n",
    "\n",
    "custom_hot = plotly.colors.sequential.Hot\n",
    "custom_hot[0] = '#000000'  # Set black for 0 values\n",
    "\n",
    "custom_plotly3 = np.array(plotly.colors.sequential.Inferno)\n",
    "custom_plotly3[0] = '#000000'  # Set black for 0 values\n",
    "\n",
    "custom_aggrnyl = np.array(plotly.colors.sequential.Aggrnyl)\n",
    "custom_aggrnyl[0] = '#000000'  # Set black for 0 values\n",
    "\n",
    "custom_magma = np.array(plotly.colors.sequential.Magma)\n",
    "custom_magma[0] = '#000000'  # Set black for 0 values\n",
    "\n",
    "custom_cividis = np.array(plotly.colors.sequential.Cividis)\n",
    "custom_cividis[0] = '#000000'  # Set black for 0 values\n",
    "\n",
    "custom_plasma = np.array(plotly.colors.sequential.Plasma)\n",
    "custom_plasma[0] = '#000000'  # Set black for 0 values\n",
    "\n",
    "custom_jet = np.array(plotly.colors.sequential.Jet)\n",
    "custom_jet[0] = '#000000'  # Set black for 0 values"
   ],
   "id": "e578e15c74a82374",
   "outputs": [],
   "execution_count": 10
  },
  {
   "cell_type": "code",
   "source": [
    "# Create subplots with 1 row and 3 columns, increased horizontal spacing\n",
    "fig = make_subplots(rows=3, cols=8, horizontal_spacing=0.01, vertical_spacing=0.01, \n",
    "                    subplot_titles=['t1 [ms]', 't2 [ms]', 'MT fss (%)',  'MT kssw [Hz]', 'Amide fs [mM]', 'Amide [Hz]', 'Glu [mM]', 'Glu ksw [Hz]'])\n",
    "\n",
    "# for subject_i, subject_dict in enumerate(subject_dicts):\n",
    "for day_i, subject_dicts in enumerate([subject_dicts_june_tumor_10, subject_dicts_june_tumor_14, subject_dicts_june_tumor_17]):\n",
    "    row_i = day_i+1\n",
    "    subject_dict = subject_dicts[subject_i]\n",
    "    phantom_choice = subject_i+1\n",
    "    subject_dict['save_name'] = save_name\n",
    "    glu_mouse_fn = os.path.join(parent_dir, 'data', 'scans', subject_dict['scan_name'], subject_dict['sub_name'])\n",
    "    \n",
    "    mask = mask_processor(glu_mouse_fn, subject_dict['resratio'])\n",
    "    subject_image_path, subject_mt_nn_image_path, subject_glu_amide_nn_image_path = image_path_creator(save_name, phantom_choice, dict_name_category, fp_prtcl_name, ga_net_name)\n",
    "    \n",
    "    # processing:\n",
    "    c_acq_data, w_acq_data, acquired_data_52, acquired_data_51, acquired_data_107, acquired_map_t1w, acquired_map_t2w = data_preparation(subject_dict, glu_mouse_fn, txt_file_name, dtype, device, sched_iter, M0_flag)\n",
    "    loss_per_epoch_mt, input_mt_param, quant_maps_mt = mt_step(mt_nn_fn, acquired_map_t1w, acquired_map_t2w, acquired_data_52, c_acq_data, w_acq_data, sched_iter, device)\n",
    "    loss_per_epoch, quant_maps_glu, quant_maps_amide = ga_step(ga_nn_fn, acquired_map_t1w, acquired_map_t2w, acquired_data_51, acquired_data_107, input_mt_param, c_acq_data, w_acq_data, sched_iter, device)\n",
    "    \n",
    "    # crop image\n",
    "    r_c, c_c = regionprops(mask)[0].centroid\n",
    "    r_c, c_c = int(r_c), int(c_c)\n",
    "    bb_r_h = 15\n",
    "    bb_c_w = 25\n",
    "    \n",
    "    if os.path.exists(os.path.join(glu_mouse_fn, 'slicer_masks', 'tumor.seg.nrrd')):\n",
    "        tumor_mask, mirror_mask = tumor_masks_processor(glu_mouse_fn, subject_dict['resratio'])\n",
    "        tumor_mask = tumor_mask[r_c-bb_r_h:r_c+bb_r_h, c_c-bb_c_w:c_c+bb_c_w]\n",
    "        mirror_mask = mirror_mask[r_c-bb_r_h:r_c+bb_r_h, c_c-bb_c_w:c_c+bb_c_w]\n",
    "        \n",
    "        tumor_cntr = contour_finder(tumor_mask)  # not accurate needs fixing!\n",
    "        mirror_cntr = contour_finder(mirror_mask)\n",
    "        \n",
    "    mask = mask\n",
    "    cropped_mask = mask[r_c-bb_r_h:r_c+bb_r_h, c_c-bb_c_w:c_c+bb_c_w]\n",
    "\n",
    "    glu_mouse_fn = os.path.join(parent_dir, 'data', 'scans', subject_dict['scan_name'], subject_dict['sub_name'])\n",
    "    highres_fn, _, bruker_dataset_highres = bruker_dataset_creator(glu_mouse_fn, txt_file_name, 'T2_MSME_high')\n",
    "    # wassr_fn, _, bruker_dataset_wassr = bruker_dataset_creator(glu_mouse_fn, txt_file_name, 'WASSR')\n",
    "    mt_fn, _, bruker_dataset_mt = bruker_dataset_creator(glu_mouse_fn, txt_file_name, 'MT52')\n",
    "    amide_fn, _, bruker_dataset_amide = bruker_dataset_creator(glu_mouse_fn, txt_file_name, '51_Amide')\n",
    "    glu_107a_fn, glu_mouse_mrf_files_fn, bruker_dataset_107a = bruker_dataset_creator(glu_mouse_fn, txt_file_name, '107a')\n",
    "    \n",
    "    # full_mask = subject_dict['roi_mask']\n",
    "    date = subject_dict['month']\n",
    "    save_name = subject_dict['save_name']\n",
    "    sub_name = subject_dict['sub_name']\n",
    "    temp = subject_dict['temp']\n",
    "    highres_img_idx = subject_dict['highres_img_idx']\n",
    "    resratio = subject_dict['resratio']\n",
    "    \n",
    "    # highres_img_idx = 4\n",
    "    t1 = t1_t2_pixel_reader(glu_phantom_fn=glu_mouse_fn, txt_file_name=txt_file_name, image_idx=3, t_type='t1',\n",
    "                                   image_file=4)[r_c-bb_r_h:r_c+bb_r_h, c_c-bb_c_w:c_c+bb_c_w]\n",
    "    t2 = t1_t2_pixel_reader(glu_phantom_fn=glu_mouse_fn, txt_file_name=txt_file_name, image_idx=3, t_type='t2')[r_c-bb_r_h:r_c+bb_r_h, c_c-bb_c_w:c_c+bb_c_w]\n",
    "    \n",
    "    # Loading the separately acquired water_t1t2-maps\n",
    "    shift_up, shift_right = subject_dict['t_shift']\n",
    "    t1 = np.roll(t1, shift=(shift_up, shift_right), axis=(0, 1))\n",
    "    t2 = np.roll(t2, shift=(shift_up, shift_right), axis=(0, 1))\n",
    "    \n",
    "    # Add heatmaps for the three arrays\n",
    "    heatmap_t1 = go.Heatmap(z=t1*cropped_mask, showscale=False, zmin=1300, zmax=2300, colorscale=custom_plotly3, coloraxis='coloraxis1') # 500-2400?\n",
    "    heatmap_t2 = go.Heatmap(z=t2*cropped_mask,showscale=False, zmin=50, zmax=110, colorscale=custom_aggrnyl, coloraxis='coloraxis2') # 35-200?\n",
    "    heatmap_mt_fs = go.Heatmap(z=quant_maps_mt['fs'][r_c-bb_r_h:r_c+bb_r_h, c_c-bb_c_w:c_c+bb_c_w]*100*cropped_mask, colorscale=custom_viridis, coloraxis='coloraxis3')\n",
    "    heatmap_mt_ksw = go.Heatmap(z=quant_maps_mt['ksw'][r_c-bb_r_h:r_c+bb_r_h, c_c-bb_c_w:c_c+bb_c_w]*cropped_mask, colorscale=custom_plasma, coloraxis='coloraxis4')\n",
    "    heatmap_amide_fs = go.Heatmap(z=quant_maps_amide['fs'][r_c-bb_r_h:r_c+bb_r_h, c_c-bb_c_w:c_c+bb_c_w]*110000*cropped_mask, colorscale=custom_viridis, coloraxis='coloraxis5')\n",
    "    heatmap_amide_ksw = go.Heatmap(z=quant_maps_amide['ksw'][r_c-bb_r_h:r_c+bb_r_h, c_c-bb_c_w:c_c+bb_c_w]*cropped_mask, colorscale=custom_plasma, coloraxis='coloraxis6')\n",
    "    heatmap_glu_fs = go.Heatmap(z=quant_maps_glu['fs'][r_c-bb_r_h:r_c+bb_r_h, c_c-bb_c_w:c_c+bb_c_w]*(110000/3)*cropped_mask, colorscale=custom_viridis, coloraxis='coloraxis7')\n",
    "    heatmap_glu_ksw = go.Heatmap(z=quant_maps_glu['ksw'][r_c-bb_r_h:r_c+bb_r_h, c_c-bb_c_w:c_c+bb_c_w]*cropped_mask, colorscale=custom_plasma, coloraxis='coloraxis8')\n",
    "\n",
    "    \n",
    "    fig.add_trace(heatmap_t1, row=row_i, col=1)\n",
    "    fig.add_trace(heatmap_t2, row=row_i, col=2)\n",
    "    fig.add_trace(heatmap_mt_fs, row=row_i, col=3)\n",
    "    fig.add_trace(heatmap_mt_ksw, row=row_i, col=4)\n",
    "    fig.add_trace(heatmap_amide_fs, row=row_i, col=5)\n",
    "    fig.add_trace(heatmap_amide_ksw, row=row_i, col=6)\n",
    "    fig.add_trace(heatmap_glu_fs, row=row_i, col=7)\n",
    "    fig.add_trace(heatmap_glu_ksw, row=row_i, col=8)\n",
    "\n",
    "fig.update_layout(\n",
    "    template='plotly_dark',  # Set the theme to plotly dark\n",
    "    title_text=f\"Mouse {phantom_choice} {date} {temp}°C\",\n",
    "    showlegend=False,  # Hide legend\n",
    "    height=450,\n",
    "    width=1400,  # Set a width based on your preference\n",
    "    margin=dict(l=5, r=40, t=60, b=60),  # Adjust top and bottom margins\n",
    "    title=dict(x=0.02, y=0.97)  # Adjust the title position\n",
    ")\n",
    "\n",
    "# Manually add separate colorbars\n",
    "colorbar_t1w = {'colorscale': custom_hot, 'cmin': 1200, 'cmax': 2300}\n",
    "colorbar_t2w = {'colorscale': custom_aggrnyl, 'cmin': 40, 'cmax': 150}\n",
    "colorbar_fs_mt = {'colorscale': custom_viridis, 'cmin': 0, 'cmax': 30}\n",
    "colorbar_ksw_mt = {'colorscale': custom_magma, 'cmin': 0, 'cmax': 70}\n",
    "colorbar_fs_amide = {'colorscale': custom_viridis, 'cmin': 0, 'cmax': 1000}\n",
    "colorbar_ksw_amide = {'colorscale': custom_magma, 'cmin': 0, 'cmax': 100}\n",
    "colorbar_fs = {'colorscale': custom_viridis, 'cmin': 0, 'cmax': 500}\n",
    "colorbar_ksw = {'colorscale': custom_magma, 'cmin': 5000, 'cmax': 10800}\n",
    "\n",
    "n = 8\n",
    "start = 1/(n*2)\n",
    "step = (1) / n\n",
    "fig.update_layout(\n",
    "    coloraxis1=colorbar_t1w,\n",
    "    coloraxis2=colorbar_t2w,\n",
    "    coloraxis3=colorbar_fs_mt,\n",
    "    coloraxis4=colorbar_ksw_mt,\n",
    "    coloraxis5=colorbar_fs_amide,\n",
    "    coloraxis6=colorbar_ksw_amide,\n",
    "    coloraxis7=colorbar_fs,\n",
    "    coloraxis8=colorbar_ksw,\n",
    "    coloraxis1_colorbar=dict(orientation='h', x=start, y=-0.12, len=1/8, thickness=18),\n",
    "    coloraxis2_colorbar=dict(orientation='h', x=start+step, y=-0.12, len=1/8, thickness=18),\n",
    "    coloraxis3_colorbar=dict(orientation='h', x=start+2*step, y=-0.12, len=1/8, thickness=18),\n",
    "    coloraxis4_colorbar=dict(orientation='h', x=start+3*step, y=-0.12, len=1/8, thickness=18),\n",
    "    coloraxis5_colorbar=dict(orientation='h', x=start+4*step, y=-0.12, len=1/8, thickness=18),\n",
    "    coloraxis6_colorbar=dict(orientation='h', x=start+5*step, y=-0.12, len=1/8, thickness=18),\n",
    "    coloraxis7_colorbar=dict(orientation='h', x=start+6*step, y=-0.12, len=1/8, thickness=18),\n",
    "    coloraxis8_colorbar=dict(orientation='h', x=start+7*step, y=-0.12, len=1/8, thickness=18),\n",
    ")\n",
    "\n",
    "# Add individual titles and separate colorbars\n",
    "for i, title in enumerate([1, 2, 3, 4, 5, 6, 7, 8], start=1):\n",
    "    fig.update_xaxes(row=1, col=i, showgrid=False, showticklabels=False)\n",
    "    fig.update_xaxes(row=2, col=i, showgrid=False, showticklabels=False)\n",
    "    fig.update_xaxes(row=3, col=i, showgrid=False, showticklabels=False)\n",
    "    fig.update_yaxes(showgrid=False, row=1, col=i, showticklabels=False, autorange='reversed')  # Reverse the y-axis , autorange='reversed'\n",
    "    fig.update_yaxes(showgrid=False, row=2, col=i, showticklabels=False, autorange='reversed')  # Reverse the y-axis\n",
    "    fig.update_yaxes(showgrid=False, row=3, col=i, showticklabels=False, autorange='reversed')  # Reverse the y-axis\n",
    "\n",
    "fig.update_yaxes(title_text='10 days', showgrid=False, row=1, col=1)\n",
    "fig.update_yaxes(title_text='14 days', showgrid=False, row=2, col=1)\n",
    "fig.update_yaxes(title_text='17 days', showgrid=False, row=3, col=1)\n",
    "# Show the plot\n",
    "fig.show()\n",
    "pio.write_image(fig, f'{subject_glu_amide_nn_image_path}/result.jpeg')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-31T11:54:33.000357Z",
     "start_time": "2024-07-31T11:54:32.499466Z"
    }
   },
   "id": "397c264c286db253",
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/hagar/mrf_final/data/scans/24_06_10_glu_tumor_mouse_37deg/8_mouse_2R_tumor_day_10/slicer_masks/mask.seg.nrrd'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[11], line 13\u001B[0m\n\u001B[1;32m     10\u001B[0m subject_dict[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msave_name\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m save_name\n\u001B[1;32m     11\u001B[0m glu_mouse_fn \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(parent_dir, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mscans\u001B[39m\u001B[38;5;124m'\u001B[39m, subject_dict[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mscan_name\u001B[39m\u001B[38;5;124m'\u001B[39m], subject_dict[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msub_name\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[0;32m---> 13\u001B[0m mask \u001B[38;5;241m=\u001B[39m \u001B[43mmask_processor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mglu_mouse_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msubject_dict\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mresratio\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     14\u001B[0m subject_image_path, subject_mt_nn_image_path, subject_glu_amide_nn_image_path \u001B[38;5;241m=\u001B[39m image_path_creator(save_name, phantom_choice, dict_name_category, fp_prtcl_name, ga_net_name)\n\u001B[1;32m     16\u001B[0m \u001B[38;5;66;03m# processing:\u001B[39;00m\n",
      "File \u001B[0;32m~/mrf_final/molecular-mrf/my_funcs/mask_functions.py:112\u001B[0m, in \u001B[0;36mmask_processor\u001B[0;34m(glu_mouse_fn, resratio, idx)\u001B[0m\n\u001B[1;32m    110\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmask_processor\u001B[39m(glu_mouse_fn, resratio, idx\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m):\n\u001B[1;32m    111\u001B[0m     mask_fn \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mjoin(glu_mouse_fn, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mslicer_masks\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmask.seg.nrrd\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m--> 112\u001B[0m     mask \u001B[38;5;241m=\u001B[39m \u001B[43mnrrd\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmask_fn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mindex_order\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mC\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m][idx, : ,:]\n\u001B[1;32m    114\u001B[0m     mask \u001B[38;5;241m=\u001B[39m downsampler(mask, resratio)\n\u001B[1;32m    116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m mask\n",
      "File \u001B[0;32m~/anaconda3/envs/molecular-new/lib/python3.9/site-packages/nrrd/reader.py:515\u001B[0m, in \u001B[0;36mread\u001B[0;34m(filename, custom_field_map, index_order)\u001B[0m\n\u001B[1;32m    480\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mread\u001B[39m(filename: \u001B[38;5;28mstr\u001B[39m, custom_field_map: Optional[NRRDFieldMap] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, index_order: IndexOrder \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mF\u001B[39m\u001B[38;5;124m'\u001B[39m) \\\n\u001B[1;32m    481\u001B[0m         \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tuple[npt\u001B[38;5;241m.\u001B[39mNDArray, NRRDHeader]:\n\u001B[1;32m    482\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Read a NRRD file and return the header and data\u001B[39;00m\n\u001B[1;32m    483\u001B[0m \n\u001B[1;32m    484\u001B[0m \u001B[38;5;124;03m    See :ref:`background/how-to-use:reading nrrd files` for more information on reading NRRD files.\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    512\u001B[0m \u001B[38;5;124;03m    :meth:`write`, :meth:`read_header`, :meth:`read_data`\u001B[39;00m\n\u001B[1;32m    513\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 515\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28;43mopen\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mfilename\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mrb\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mas\u001B[39;00m fh:\n\u001B[1;32m    516\u001B[0m         header \u001B[38;5;241m=\u001B[39m read_header(fh, custom_field_map)\n\u001B[1;32m    517\u001B[0m         data \u001B[38;5;241m=\u001B[39m read_data(header, fh, filename, index_order)\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '/home/hagar/mrf_final/data/scans/24_06_10_glu_tumor_mouse_37deg/8_mouse_2R_tumor_day_10/slicer_masks/mask.seg.nrrd'"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "# Plotting training loss\n",
    "plt.figure()\n",
    "plt.plot(loss_per_epoch_mt[0:])\n",
    "plt.xlabel('Epoch', fontsize=20)\n",
    "plt.ylabel('MSE Loss', fontsize=20)\n",
    "plt.title('Training Loss (MT)', fontsize=20)\n",
    "plt.tight_layout()  # Adjust subplot parameters to give specified padding\n",
    "plt.savefig(f'{subject_glu_amide_nn_image_path}/mt_loss.jpeg', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "74523057607990fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Plotting training loss\n",
    "plt.figure()\n",
    "plt.plot(loss_per_epoch[0:])\n",
    "plt.xlabel('Epoch', fontsize=20)\n",
    "plt.ylabel('MSE Loss', fontsize=20)\n",
    "plt.title('Training Loss (Glu+Amide)', fontsize=20)\n",
    "plt.tight_layout()  # Adjust subplot parameters to give specified padding\n",
    "plt.savefig(f'{subject_glu_amide_nn_image_path}/glu_amide_loss.jpeg', bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b682d15fedc3ffd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "c9492ba5a38d876c",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
